# SPDX-FileCopyrightText: 2022-present deepset GmbH <info@deepset.ai>
#
# SPDX-License-Identifier: Apache-2.0

# pylint: disable=protected-access

from typing import Dict, Any, List
import json
import logging
from pathlib import Path
from collections import OrderedDict

from canals.component.component import component, Component
from canals.pipeline.pipeline import Pipeline


logger = logging.getLogger(__name__)


def _json_writer(data: Dict[str, Any], stream):
    return json.dump(data, stream, indent=4)


def save_pipelines(pipelines: Dict[str, Pipeline], path: Path, writer=_json_writer) -> None:
    """
    Converts a dictionary of named Pipelines into a JSON file.

    Args:
        pipelines: dictionary of {name: pipeline_object}
        path: where to write the resulting file
        writer: which function to use to write the dictionary to a file.
            Use this parameter to dump to a different format like YAML, TOML, HCL, etc.

    Returns:
        None
    """
    data = marshal_pipelines(pipelines=pipelines)
    with open(path, "w", encoding="utf-8") as file:
        writer(data, file)


def load_pipelines(path: Path, reader=json.load) -> Dict[str, Pipeline]:
    """
    Loads the content of a JSON file generated by `save_pipelines()` into
    a dictionary of named Pipelines.

    Args:
        path: where to read the file from
        reader: which function to use to read the dictionary to a file.
            Use this parameter to load from a different format like YAML, TOML, HCL, etc.

    Returns:
        The pipelines as a dictionary of `{"pipeline-name": <pipeline object>}`
    """
    with open(path, "r", encoding="utf-8") as handle:
        data = reader(handle)
    return unmarshal_pipelines(data)


def marshal_pipelines(pipelines: Dict[str, Pipeline]) -> Dict[str, Any]:
    """
    Converts a dictionary of named Pipelines into a Python dictionary that can be
    written to a JSON file.

    In case there are different component instances, meaning components with a different
    hash, and an identical name they are suffixed with an underscore and an incrementing number.
    This way we can be sure that there is no confusion when loading back the Pipelines from file.
    Obviously names will be different when unmarshaling but Pipelines' behaviour won't change.

    Args:
        pipelines: A dictionary of `{"pipeline-name": <pipeline object>}`

    Returns:
        A Python dictionary representing the Pipelines objects above, that can be written to JSON and can be reused to
        recreate the original Pipelines.
    """
    data = {}
    data["pipelines"] = {name: p.to_dict() for name, p in pipelines.items()}

    # Mapping of each name to a list of instance hashes.
    # This let us know if a name refers to different components.
    name_to_hashes: Dict[str, List[int]] = {}
    # Mapping of each component hash to its component.
    # Not strictly necessary but makes it faster to get a component.
    hash_to_component: Dict[int, Dict[str, Any]] = {}

    for pipeline in data["pipelines"].values():
        for comp_name, comp_data in OrderedDict(pipeline["components"]).items():
            if comp_name not in name_to_hashes:
                name_to_hashes[comp_name] = []
            hash_ = comp_data["hash"]
            if hash_ not in name_to_hashes[comp_name]:
                name_to_hashes[comp_name].append(hash_)

            hash_to_component[hash_] = comp_data

    connections_renames = {}
    data["components"] = {}

    for name, hashes in name_to_hashes.items():
        if len(hashes) > 1:
            # Multiple different instances with this name, we rename them
            for index, hash_ in enumerate(hashes):
                new_name = f"{name}_{index+1}"
                connections_renames[new_name] = name
                data["components"][new_name] = hash_to_component[hash_]
            continue

        hash_ = list(hashes)[0]
        data["components"][name] = hash_to_component[hash_]

    _rename_connections(data, connections_renames)
    _remove_duplicate_instances(data["components"])
    _remove_pipeline_component_data(data)
    _remove_component_hashes(data)

    return data


def _rename_connections(data: Dict[str, Any], renames: Dict[str, str]):
    """
    Rename all connections of all Pipelines found in data using renames.
    """
    for new_name, old_name in renames.items():
        for pipe in data["pipelines"].values():
            if old_name not in pipe["components"]:
                # The component to rename is not in this pipeline
                continue

            if pipe["components"][old_name]["hash"] != data["components"][new_name]["hash"]:
                # The name matches but it's another instance, it will be renamed to
                # some other name
                continue

            for connection in pipe["connections"]:
                # We split from the right just in case there is a dot in the
                # component name, socket names shouldn't contain names so it's
                # less risky this way.
                sender = connection["sender"].rsplit(".", maxsplit=1)
                receiver = connection["receiver"].rsplit(".", maxsplit=1)
                if sender[0] == old_name:
                    sender[0] = sender[0].replace(old_name, new_name)
                if receiver[0] == old_name:
                    receiver[0] = receiver[0].replace(old_name, new_name)
                connection["sender"] = ".".join(sender)
                connection["receiver"] = ".".join(receiver)


def _remove_duplicate_instances(components: Dict[str, Any]):
    """
    Remove duplicate declaration of the same component instance.
    If two or more components have the same hash remove all duplicates
    and point them to a single component.

    A structure like this:
    ```
    {
        "comp1": {"hash": 123, ...},
        "comp2": {"hash": 123, ...},
        "comp3": {"hash": 456, ...}
    }
    ```

    will become:
    ```
    {
        "comp1": {"hash": 123, ...},
        "comp2": "comp1",
        "comp3": {"hash": 456, ...}
    }
    ```
    """
    for comp_name, comp in components.items():
        if isinstance(comp, str):
            # This component is just a pointer to another one
            continue
        for other_comp_name, other_comp in components.items():
            if other_comp_name == comp_name:
                # It's the same component, skip it
                continue
            if isinstance(other_comp, str):
                # This component is just a pointer to another one
                continue
            if other_comp["hash"] == comp["hash"]:
                components[other_comp_name] = comp_name


def _remove_pipeline_component_data(data: Dict[str, Any]):
    """
    Delete components declared in each pipeline.
    Connections are enough since we're declaring components globally.
    """
    for pipe in data["pipelines"].values():
        del pipe["components"]


def _remove_component_hashes(data: Dict[str, Any]):
    """
    Delete components hashes, we don't need them anymore.
    """
    for comp in data["components"].values():
        if isinstance(comp, dict):
            del comp["hash"]


def unmarshal_pipelines(data: Dict[str, Any]) -> Dict[str, Pipeline]:
    """
    Loads the content of a dictionary generated by `marshal_pipelines()` into
    a dictionary of named Pipelines.

    Args:
        data: pipelines' data, as generated by `marshal_pipelines()`.

    Returns:
        The pipelines as a dictionary of `{"pipeline-name": <pipeline object>}`.
    """
    components = _unmarshal_components(data)
    pipelines = _unmarshal_pipelines(data, components)
    return pipelines


def _unmarshal_components(data: Dict[str, Any]) -> Dict[str, Component]:
    """
    Loads the components from the given Pipeline data
    """

    components: Dict[str, Component] = {}
    # Create a mapping of component names and component instances
    for name, comp_data in data["components"].items():
        if isinstance(comp_data, str):
            # This component refers to another one
            components[name] = components[comp_data]
            continue
        component_class = component.registry[comp_data["type"]]
        components[name] = component_class.from_dict(comp_data)
    return components


def _unmarshal_pipelines(data: Dict[str, Any], components: Dict[str, Component]) -> Dict[str, Pipeline]:
    """
    Loads the Pipelines, given the data and the components already loaded
    """
    pipelines = {}
    for name, pipe_data in data["pipelines"].items():
        # Add back the components as empty dicts.
        # Pipeline.from_dict() will use them to get instances from the
        # components deserialized above.
        # If we don't add them back Pipeline.from_dict() will fail when deserializing
        # connections as the Pipeline instance won't have any component.
        pipe_data["components"] = {}
        for connection in pipe_data["connections"]:
            sender = connection["sender"].rsplit(".")[0]
            receiver = connection["receiver"].rsplit(".")[0]
            pipe_data["components"][sender] = {}
            pipe_data["components"][receiver] = {}

        pipelines[name] = Pipeline.from_dict(pipe_data, components=components)
    return pipelines
