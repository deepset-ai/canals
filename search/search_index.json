{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Canals","text":"<p>Canals is a component orchestration engine. Components are Python objects that can execute a task, like reading a file, performing calculations, or making API calls. Canals connects these objects together: it builds a graph of components and takes care of managing their execution order, making sure that each object receives the input it expects from the other components of the pipeline.</p> <p>Canals powers version 2.0 of Haystack.</p>"},{"location":"#installation","title":"Installation","text":"<p>Running:</p> <pre><code>pip install canals\n</code></pre> <p>gives you the bare minimum necessary to run Canals.</p> <p>To be able to draw pipelines, please make sure you have either an internet connection (to reach the Mermaid graph renderer at <code>https://mermaid.ink</code>) or graphviz installed and then install Canals as:</p>"},{"location":"#mermaid","title":"Mermaid","text":"<pre><code>pip install canals[mermaid]\n</code></pre>"},{"location":"#graphviz","title":"GraphViz","text":"<pre><code>sudo apt install graphviz  # You may need `graphviz-dev` too\npip install canals[graphviz]\n</code></pre>"},{"location":"api-docs/canals/","title":"Canals","text":""},{"location":"api-docs/canals/#canals.Component","title":"<code>Component</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Note this is only used by type checking tools.</p> <p>In order to implement the <code>Component</code> protocol, custom components need to have a <code>run</code> method. The signature of the method and its return value won't be checked, i.e. classes with the following methods:</p> <pre><code>def run(self, param: str) -&gt; Dict[str, Any]:\n    ...\n</code></pre> <p>and</p> <pre><code>def run(self, **kwargs):\n    ...\n</code></pre> <p>will be both considered as respecting the protocol. This makes the type checking much weaker, but we have other places where we ensure code is dealing with actual Components.</p> <p>The protocol is runtime checkable so it'll be possible to assert:</p> <pre><code>isinstance(MyComponent, Component)\n</code></pre> Source code in <code>canals/component/component.py</code> <pre><code>@runtime_checkable\nclass Component(Protocol):\n    \"\"\"\n    Note this is only used by type checking tools.\n\n    In order to implement the `Component` protocol, custom components need to\n    have a `run` method. The signature of the method and its return value\n    won't be checked, i.e. classes with the following methods:\n\n        def run(self, param: str) -&gt; Dict[str, Any]:\n            ...\n\n    and\n\n        def run(self, **kwargs):\n            ...\n\n    will be both considered as respecting the protocol. This makes the type\n    checking much weaker, but we have other places where we ensure code is\n    dealing with actual Components.\n\n    The protocol is runtime checkable so it'll be possible to assert:\n\n        isinstance(MyComponent, Component)\n    \"\"\"\n\n    def run(self, *args: Any, **kwargs: Any):  # pylint: disable=missing-function-docstring\n        ...\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline","title":"<code>Pipeline</code>","text":"<p>Components orchestration engine.</p> <p>Builds a graph of components and orchestrates their execution according to the execution graph.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"\n    Components orchestration engine.\n\n    Builds a graph of components and orchestrates their execution according to the execution graph.\n    \"\"\"\n\n    def __init__(\n        self,\n        metadata: Optional[Dict[str, Any]] = None,\n        max_loops_allowed: int = 100,\n        debug_path: Union[Path, str] = Path(\".canals_debug/\"),\n    ):\n        \"\"\"\n        Creates the Pipeline.\n\n        Args:\n            metadata: arbitrary dictionary to store metadata about this pipeline. Make sure all the values contained in\n                this dictionary can be serialized and deserialized if you wish to save this pipeline to file with\n                `save_pipelines()/load_pipelines()`.\n            max_loops_allowed: how many times the pipeline can run the same node before throwing an exception.\n            debug_path: when debug is enabled in `run()`, where to save the debug data.\n        \"\"\"\n        self.metadata = metadata or {}\n        self.max_loops_allowed = max_loops_allowed\n        self.graph = networkx.MultiDiGraph()\n        self._connections: List[Connection] = []\n        self._mandatory_connections: Dict[str, List[Connection]] = defaultdict(list)\n        self._debug: Dict[int, Dict[str, Any]] = {}\n        self._debug_path = Path(debug_path)\n\n    def __eq__(self, other) -&gt; bool:\n        \"\"\"\n        Equal pipelines share every metadata, node and edge, but they're not required to use\n        the same node instances: this allows pipeline saved and then loaded back to be equal to themselves.\n        \"\"\"\n        if (\n            not isinstance(other, type(self))\n            or not getattr(self, \"metadata\") == getattr(other, \"metadata\")\n            or not getattr(self, \"max_loops_allowed\") == getattr(other, \"max_loops_allowed\")\n            or not hasattr(self, \"graph\")\n            or not hasattr(other, \"graph\")\n        ):\n            return False\n\n        return (\n            self.graph.adj == other.graph.adj\n            and self._comparable_nodes_list(self.graph) == self._comparable_nodes_list(other.graph)\n            and self.graph.graph == other.graph.graph\n        )\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Returns this Pipeline instance as a dictionary.\n        This is meant to be an intermediate representation but it can be also used to save a pipeline to file.\n        \"\"\"\n        components = {}\n        for name, instance in self.graph.nodes(data=\"instance\"):  # type:ignore\n            components[name] = component_to_dict(instance)\n\n        connections = []\n        for sender, receiver, edge_data in self.graph.edges.data():\n            sender_socket = edge_data[\"from_socket\"].name\n            receiver_socket = edge_data[\"to_socket\"].name\n            connections.append({\"sender\": f\"{sender}.{sender_socket}\", \"receiver\": f\"{receiver}.{receiver_socket}\"})\n        return {\n            \"metadata\": self.metadata,\n            \"max_loops_allowed\": self.max_loops_allowed,\n            \"components\": components,\n            \"connections\": connections,\n        }\n\n    @classmethod\n    def from_dict(cls: Type[T], data: Dict[str, Any], **kwargs) -&gt; T:\n        \"\"\"\n        Creates a Pipeline instance from a dictionary.\n        A sample `data` dictionary could be formatted like so:\n        ```\n        {\n            \"metadata\": {\"test\": \"test\"},\n            \"max_loops_allowed\": 100,\n            \"components\": {\n                \"add_two\": {\n                    \"type\": \"AddFixedValue\",\n                    \"init_parameters\": {\"add\": 2},\n                },\n                \"add_default\": {\n                    \"type\": \"AddFixedValue\",\n                    \"init_parameters\": {\"add\": 1},\n                },\n                \"double\": {\n                    \"type\": \"Double\",\n                },\n            },\n            \"connections\": [\n                {\"sender\": \"add_two.result\", \"receiver\": \"double.value\"},\n                {\"sender\": \"double.value\", \"receiver\": \"add_default.value\"},\n            ],\n        }\n        ```\n\n        Supported kwargs:\n        `components`: a dictionary of {name: instance} to reuse instances of components instead of creating new ones.\n        \"\"\"\n        metadata = data.get(\"metadata\", {})\n        max_loops_allowed = data.get(\"max_loops_allowed\", 100)\n        debug_path = Path(data.get(\"debug_path\", \".canals_debug/\"))\n        pipe = cls(metadata=metadata, max_loops_allowed=max_loops_allowed, debug_path=debug_path)\n        components_to_reuse = kwargs.get(\"components\", {})\n        for name, component_data in data.get(\"components\", {}).items():\n            if name in components_to_reuse:\n                # Reuse an instance\n                instance = components_to_reuse[name]\n            else:\n                if \"type\" not in component_data:\n                    raise PipelineError(f\"Missing 'type' in component '{name}'\")\n\n                if component_data[\"type\"] not in component.registry:\n                    try:\n                        # Import the module first...\n                        module, _ = component_data[\"type\"].rsplit(\".\", 1)\n                        logger.debug(\"Trying to import %s\", module)\n                        importlib.import_module(module)\n                        # ...then try again\n                        if component_data[\"type\"] not in component.registry:\n                            raise PipelineError(\n                                f\"Successfully imported module {module} but can't find it in the component registry.\"\n                                \"This is unexpected and most likely a bug.\"\n                            )\n                    except (ImportError, PipelineError) as e:\n                        raise PipelineError(f\"Component '{component_data['type']}' not imported.\") from e\n\n                # Create a new one\n                component_class = component.registry[component_data[\"type\"]]\n                instance = component_from_dict(component_class, component_data)\n            pipe.add_component(name=name, instance=instance)\n\n        for connection in data.get(\"connections\", []):\n            if \"sender\" not in connection:\n                raise PipelineError(f\"Missing sender in connection: {connection}\")\n            if \"receiver\" not in connection:\n                raise PipelineError(f\"Missing receiver in connection: {connection}\")\n            pipe.connect(connect_from=connection[\"sender\"], connect_to=connection[\"receiver\"])\n\n        return pipe\n\n    def _comparable_nodes_list(self, graph: networkx.MultiDiGraph) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Replaces instances of nodes with their class name in order to make sure they're comparable.\n        \"\"\"\n        nodes = []\n        for node in graph.nodes:\n            comparable_node = graph.nodes[node]\n            comparable_node[\"instance\"] = comparable_node[\"instance\"].__class__\n            nodes.append(comparable_node)\n        nodes.sort()\n        return nodes\n\n    def add_component(self, name: str, instance: Component) -&gt; None:\n        \"\"\"\n        Create a component for the given component. Components are not connected to anything by default:\n        use `Pipeline.connect()` to connect components together.\n\n        Component names must be unique, but component instances can be reused if needed.\n\n        Args:\n            name: the name of the component.\n            instance: the component instance.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: if a component with the same name already exists\n            PipelineValidationError: if the given instance is not a Canals component\n        \"\"\"\n        # Component names are unique\n        if name in self.graph.nodes:\n            raise ValueError(f\"A component named '{name}' already exists in this pipeline: choose another name.\")\n\n        # Components can't be named `_debug`\n        if name == \"_debug\":\n            raise ValueError(\"'_debug' is a reserved name for debug output. Choose another name.\")\n\n        # Component instances must be components\n        if not isinstance(instance, Component):\n            raise PipelineValidationError(\n                f\"'{type(instance)}' doesn't seem to be a component. Is this class decorated with @component?\"\n            )\n\n        # Create the component's input and output sockets\n        input_sockets = getattr(instance, \"__canals_input__\", {})\n        output_sockets = getattr(instance, \"__canals_output__\", {})\n\n        # Add component to the graph, disconnected\n        logger.debug(\"Adding component '%s' (%s)\", name, instance)\n        self.graph.add_node(\n            name, instance=instance, input_sockets=input_sockets, output_sockets=output_sockets, visits=0\n        )\n\n    def connect(self, connect_from: str, connect_to: str) -&gt; None:\n        \"\"\"\n        Connects two components together. All components to connect must exist in the pipeline.\n        If connecting to an component that has several output connections, specify the inputs and output names as\n        'component_name.connections_name'.\n\n        Args:\n            connect_from: the component that delivers the value. This can be either just a component name or can be\n                in the format `component_name.connection_name` if the component has multiple outputs.\n            connect_to: the component that receives the value. This can be either just a component name or can be\n                in the format `component_name.connection_name` if the component has multiple inputs.\n\n        Returns:\n            None\n\n        Raises:\n            PipelineConnectError: if the two components cannot be connected (for example if one of the components is\n                not present in the pipeline, or the connections don't match by type, and so on).\n        \"\"\"\n        # Edges may be named explicitly by passing 'node_name.edge_name' to connect().\n        sender, sender_socket_name = parse_connect_string(connect_from)\n        receiver, receiver_socket_name = parse_connect_string(connect_to)\n\n        # Get the nodes data.\n        try:\n            from_sockets = self.graph.nodes[sender][\"output_sockets\"]\n        except KeyError as exc:\n            raise ValueError(f\"Component named {sender} not found in the pipeline.\") from exc\n        try:\n            to_sockets = self.graph.nodes[receiver][\"input_sockets\"]\n        except KeyError as exc:\n            raise ValueError(f\"Component named {receiver} not found in the pipeline.\") from exc\n\n        # If the name of either socket is given, get the socket\n        sender_socket: Optional[OutputSocket] = None\n        if sender_socket_name:\n            sender_socket = from_sockets.get(sender_socket_name)\n            if not sender_socket:\n                raise PipelineConnectError(\n                    f\"'{connect_from} does not exist. \"\n                    f\"Output connections of {sender} are: \"\n                    + \", \".join([f\"{name} (type {_type_name(socket.type)})\" for name, socket in from_sockets.items()])\n                )\n\n        receiver_socket: Optional[InputSocket] = None\n        if receiver_socket_name:\n            receiver_socket = to_sockets.get(receiver_socket_name)\n            if not receiver_socket:\n                raise PipelineConnectError(\n                    f\"'{connect_to} does not exist. \"\n                    f\"Input connections of {receiver} are: \"\n                    + \", \".join([f\"{name} (type {_type_name(socket.type)})\" for name, socket in to_sockets.items()])\n                )\n\n        # Look for a matching connection among the possible ones.\n        # Note that if there is more than one possible connection but two sockets match by name, they're paired.\n        sender_socket_candidates: List[OutputSocket] = [sender_socket] if sender_socket else list(from_sockets.values())\n        receiver_socket_candidates: List[InputSocket] = (\n            [receiver_socket] if receiver_socket else list(to_sockets.values())\n        )\n\n        connection = Connection.from_list_of_sockets(\n            sender, sender_socket_candidates, receiver, receiver_socket_candidates\n        )\n\n        # Connection must be valid on both sender/receiver sides\n        if (\n            not connection.sender_socket\n            or not connection.receiver_socket\n            or not connection.sender\n            or not connection.receiver\n        ):\n            raise PipelineConnectError(\"Connection must have both sender and receiver: {connection}\")\n\n        # Create the connection\n        logger.debug(\n            \"Connecting '%s.%s' to '%s.%s'\",\n            connection.sender,\n            connection.sender_socket.name,\n            connection.receiver,\n            connection.receiver_socket.name,\n        )\n\n        self.graph.add_edge(\n            connection.sender,\n            connection.receiver,\n            key=f\"{connection.sender_socket.name}/{connection.receiver_socket.name}\",\n            conn_type=_type_name(connection.sender_socket.type),\n            from_socket=connection.sender_socket,\n            to_socket=connection.receiver_socket,\n        )\n\n        self._connections.append(connection)\n        if connection.is_mandatory:\n            self._mandatory_connections[connection.receiver].append(connection)\n\n    def get_component(self, name: str) -&gt; Component:\n        \"\"\"\n        Returns an instance of a component.\n\n        Args:\n            name: the name of the component\n\n        Returns:\n            The instance of that component.\n\n        Raises:\n            ValueError: if a component with that name is not present in the pipeline.\n        \"\"\"\n        try:\n            return self.graph.nodes[name][\"instance\"]\n        except KeyError as exc:\n            raise ValueError(f\"Component named {name} not found in the pipeline.\") from exc\n\n    def inputs(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Returns a dictionary containing the inputs of a pipeline. Each key in the dictionary\n        corresponds to a component name, and its value is another dictionary that describes the\n        input sockets of that component, including their types and whether they are optional.\n\n        Returns:\n            A dictionary where each key is a pipeline component name and each value is a dictionary of\n            inputs sockets of that component.\n        \"\"\"\n        inputs = {\n            comp: {socket.name: {\"type\": socket.type, \"is_mandatory\": socket.is_mandatory} for socket in data}\n            for comp, data in find_pipeline_inputs(self.graph).items()\n            if data\n        }\n        return inputs\n\n    def outputs(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Returns a dictionary containing the outputs of a pipeline. Each key in the dictionary\n        corresponds to a component name, and its value is another dictionary that describes the\n        output sockets of that component.\n\n        Returns:\n            A dictionary where each key is a pipeline component name and each value is a dictionary of\n            output sockets of that component.\n        \"\"\"\n        outputs = {\n            comp: {socket.name: {\"type\": socket.type} for socket in data}\n            for comp, data in find_pipeline_outputs(self.graph).items()\n            if data\n        }\n        return outputs\n\n    def draw(self, path: Path, engine: RenderingEngines = \"mermaid-image\") -&gt; None:\n        \"\"\"\n        Draws the pipeline. Requires either `graphviz` as a system dependency, or an internet connection for Mermaid.\n        Run `pip install canals[graphviz]` or `pip install canals[mermaid]` to install missing dependencies.\n\n        Args:\n            path: where to save the diagram.\n            engine: which format to save the graph as. Accepts 'graphviz', 'mermaid-text', 'mermaid-image'.\n                Default is 'mermaid-image'.\n\n        Returns:\n            None\n\n        Raises:\n            ImportError: if `engine='graphviz'` and `pygraphviz` is not installed.\n            HTTPConnectionError: (and similar) if the internet connection is down or other connection issues.\n        \"\"\"\n        _draw(graph=networkx.MultiDiGraph(self.graph), path=path, engine=engine)\n\n    def warm_up(self):\n        \"\"\"\n        Make sure all nodes are warm.\n\n        It's the node's responsibility to make sure this method can be called at every `Pipeline.run()`\n        without re-initializing everything.\n        \"\"\"\n        for node in self.graph.nodes:\n            if hasattr(self.graph.nodes[node][\"instance\"], \"warm_up\"):\n                logger.info(\"Warming up component %s...\", node)\n                self.graph.nodes[node][\"instance\"].warm_up()\n\n    def run(self, data: Dict[str, Any], debug: bool = False) -&gt; Dict[str, Any]:  # pylint: disable=too-many-locals\n        \"\"\"\n        Runs the pipeline.\n\n        Args:\n            data: the inputs to give to the input components of the Pipeline.\n            debug: whether to collect and return debug information.\n\n        Returns:\n            A dictionary with the outputs of the output components of the Pipeline.\n\n        Raises:\n            PipelineRuntimeError: if the any of the components fail or return unexpected output.\n        \"\"\"\n        self._clear_visits_count()\n        data = validate_pipeline_input(self.graph, input_values=data)\n        logger.info(\"Pipeline execution started.\")\n\n        self._debug = {}\n        if debug:\n            logger.info(\"Debug mode ON.\")\n            os.makedirs(\"debug\", exist_ok=True)\n\n        logger.debug(\n            \"Mandatory connections:\\n%s\",\n            \"\\n\".join(\n                f\" - {component}: {', '.join([str(s) for s in sockets])}\"\n                for component, sockets in self._mandatory_connections.items()\n            ),\n        )\n\n        self.warm_up()\n\n        # Prepare the inputs buffers and components queue\n        components_queue: List[str] = []\n        mandatory_values_buffer: Dict[Connection, Any] = {}\n        optional_values_buffer: Dict[Connection, Any] = {}\n        pipeline_output: Dict[str, Dict[str, Any]] = defaultdict(dict)\n\n        for node_name, input_data in data.items():\n            for socket_name, value in input_data.items():\n                # Make a copy of the input value so components don't need to\n                # take care of mutability.\n                value = deepcopy(value)\n                connection = Connection(\n                    None, None, node_name, self.graph.nodes[node_name][\"input_sockets\"][socket_name]\n                )\n                self._add_value_to_buffers(\n                    value, connection, components_queue, mandatory_values_buffer, optional_values_buffer\n                )\n\n        # *** PIPELINE EXECUTION LOOP ***\n        step = 0\n        while components_queue:  # pylint: disable=too-many-nested-blocks\n            step += 1\n            if debug:\n                self._record_pipeline_step(\n                    step, components_queue, mandatory_values_buffer, optional_values_buffer, pipeline_output\n                )\n\n            component_name = components_queue.pop(0)\n            logger.debug(\"&gt; Queue at step %s: %s %s\", step, component_name, components_queue)\n            self._check_max_loops(component_name)\n\n            # **** RUN THE NODE ****\n            if not self._ready_to_run(component_name, mandatory_values_buffer, components_queue):\n                continue\n\n            inputs = {\n                **self._extract_inputs_from_buffer(component_name, mandatory_values_buffer),\n                **self._extract_inputs_from_buffer(component_name, optional_values_buffer),\n            }\n            outputs = self._run_component(name=component_name, inputs=dict(inputs))\n\n            # **** PROCESS THE OUTPUT ****\n            for socket_name, value in outputs.items():\n                targets = self._collect_targets(component_name, socket_name)\n                if not targets:\n                    pipeline_output[component_name][socket_name] = value\n                else:\n                    for target in targets:\n                        self._add_value_to_buffers(\n                            value, target, components_queue, mandatory_values_buffer, optional_values_buffer\n                        )\n\n        if debug:\n            self._record_pipeline_step(\n                step + 1, components_queue, mandatory_values_buffer, optional_values_buffer, pipeline_output\n            )\n            os.makedirs(self._debug_path, exist_ok=True)\n            with open(self._debug_path / \"data.json\", \"w\", encoding=\"utf-8\") as datafile:\n                json.dump(self._debug, datafile, indent=4, default=str)\n            pipeline_output[\"_debug\"] = self._debug  # type: ignore\n\n        logger.info(\"Pipeline executed successfully.\")\n        return dict(pipeline_output)\n\n    def _record_pipeline_step(\n        self, step, components_queue, mandatory_values_buffer, optional_values_buffer, pipeline_output\n    ):\n        \"\"\"\n        Stores a snapshot of this step into the self.debug dictionary of the pipeline.\n        \"\"\"\n        mermaid_graph = _convert_for_debug(deepcopy(self.graph))\n        self._debug[step] = {\n            \"time\": datetime.datetime.now(),\n            \"components_queue\": components_queue,\n            \"mandatory_values_buffer\": mandatory_values_buffer,\n            \"optional_values_buffer\": optional_values_buffer,\n            \"pipeline_output\": pipeline_output,\n            \"diagram\": mermaid_graph,\n        }\n\n    def _clear_visits_count(self):\n        \"\"\"\n        Make sure all nodes's visits count is zero.\n        \"\"\"\n        for node in self.graph.nodes:\n            self.graph.nodes[node][\"visits\"] = 0\n\n    def _check_max_loops(self, component_name: str):\n        \"\"\"\n        Verify whether this component run too many times.\n        \"\"\"\n        if self.graph.nodes[component_name][\"visits\"] &gt; self.max_loops_allowed:\n            raise PipelineMaxLoops(\n                f\"Maximum loops count ({self.max_loops_allowed}) exceeded for component '{component_name}'.\"\n            )\n\n    def _add_value_to_buffers(\n        self,\n        value: Any,\n        connection: Connection,\n        components_queue: List[str],\n        mandatory_values_buffer: Dict[Connection, Any],\n        optional_values_buffer: Dict[Connection, Any],\n    ):\n        \"\"\"\n        Given a value and the connection it is being sent on, it updates the buffers and the components queue.\n        \"\"\"\n        if connection.is_mandatory:\n            mandatory_values_buffer[connection] = value\n            if connection.receiver and connection.receiver not in components_queue:\n                components_queue.append(connection.receiver)\n        else:\n            optional_values_buffer[connection] = value\n\n    def _ready_to_run(\n        self, component_name: str, mandatory_values_buffer: Dict[Connection, Any], components_queue: List[str]\n    ) -&gt; bool:\n        \"\"\"\n        Returns True if a component is ready to run, False otherwise.\n        \"\"\"\n        connections_with_value = set(conn for conn in mandatory_values_buffer.keys() if conn.receiver == component_name)\n        expected_connections = set(self._mandatory_connections[component_name])\n        if expected_connections.issubset(connections_with_value):\n            logger.debug(\"Component '%s' is ready to run. All mandatory values were received.\", component_name)\n            return True\n\n        # Collect the missing values still being computed we need to wait for\n        missing_connections: Set[Connection] = expected_connections - connections_with_value\n        connections_to_wait = []\n        for missing_conn in missing_connections:\n            if any(\n                networkx.has_path(self.graph, component_to_run, missing_conn.sender)\n                for component_to_run in components_queue\n            ):\n                connections_to_wait.append(missing_conn)\n\n        if not connections_to_wait:\n            # None of the missing values are needed to visit this part of the graph: we can run the component\n            logger.debug(\n                \"Component '%s' is ready to run. A variadic input parameter received all the expected values.\",\n                component_name,\n            )\n            return True\n\n        # Component can't run, waiting for the values needed by `connections_to_wait`\n        logger.debug(\n            \"Component '%s' is not ready to run, some values are still missing: %s\",\n            component_name,\n            connections_to_wait,\n        )\n        # Put the component back in the queue\n        components_queue.append(component_name)\n        return False\n\n    def _extract_inputs_from_buffer(self, component_name: str, buffer: Dict[Connection, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extract a component's input values from one of the value buffers.\n        \"\"\"\n        inputs = defaultdict(list)\n        connections: List[Connection] = []\n\n        for connection in buffer.keys():\n            if connection.receiver == component_name:\n                connections.append(connection)\n\n        for key in connections:\n            value = buffer.pop(key)\n            if key.receiver_socket:\n                if key.receiver_socket.is_variadic:\n                    inputs[key.receiver_socket.name].append(value)\n                else:\n                    inputs[key.receiver_socket.name] = value\n        return inputs\n\n    def _run_component(self, name: str, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Once we're confident this component is ready to run, run it and collect the output.\n        \"\"\"\n        self.graph.nodes[name][\"visits\"] += 1\n        instance = self.graph.nodes[name][\"instance\"]\n        try:\n            logger.info(\"* Running %s\", name)\n            logger.debug(\"   '%s' inputs: %s\", name, inputs)\n\n            outputs = instance.run(**inputs)\n\n            # Unwrap the output\n            logger.debug(\"   '%s' outputs: %s\\n\", name, outputs)\n\n            # Make sure the component returned a dictionary\n            if not isinstance(outputs, dict):\n                raise PipelineRuntimeError(\n                    f\"Component '{name}' returned a value of type '{_type_name(type(outputs))}' instead of a dict. \"\n                    \"Components must always return dictionaries: check the the documentation.\"\n                )\n\n        except Exception as e:\n            raise PipelineRuntimeError(\n                f\"{name} raised '{e.__class__.__name__}: {e}' \\nInputs: {inputs}\\n\\n\"\n                \"See the stacktrace above for more information.\"\n            ) from e\n\n        return outputs\n\n    def _collect_targets(self, component_name: str, socket_name: str) -&gt; List[Connection]:\n        \"\"\"\n        Given a component and an output socket name, return a list of Connections\n        for which they represent the sender. Used to route output.\n        \"\"\"\n        return [\n            connection\n            for connection in self._connections\n            if connection.sender == component_name\n            and connection.sender_socket\n            and connection.sender_socket.name == socket_name\n        ]\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Equal pipelines share every metadata, node and edge, but they're not required to use the same node instances: this allows pipeline saved and then loaded back to be equal to themselves.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def __eq__(self, other) -&gt; bool:\n    \"\"\"\n    Equal pipelines share every metadata, node and edge, but they're not required to use\n    the same node instances: this allows pipeline saved and then loaded back to be equal to themselves.\n    \"\"\"\n    if (\n        not isinstance(other, type(self))\n        or not getattr(self, \"metadata\") == getattr(other, \"metadata\")\n        or not getattr(self, \"max_loops_allowed\") == getattr(other, \"max_loops_allowed\")\n        or not hasattr(self, \"graph\")\n        or not hasattr(other, \"graph\")\n    ):\n        return False\n\n    return (\n        self.graph.adj == other.graph.adj\n        and self._comparable_nodes_list(self.graph) == self._comparable_nodes_list(other.graph)\n        and self.graph.graph == other.graph.graph\n    )\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.__init__","title":"<code>__init__(metadata=None, max_loops_allowed=100, debug_path=Path('.canals_debug/'))</code>","text":"<p>Creates the Pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>arbitrary dictionary to store metadata about this pipeline. Make sure all the values contained in this dictionary can be serialized and deserialized if you wish to save this pipeline to file with <code>save_pipelines()/load_pipelines()</code>.</p> <code>None</code> <code>max_loops_allowed</code> <code>int</code> <p>how many times the pipeline can run the same node before throwing an exception.</p> <code>100</code> <code>debug_path</code> <code>Union[Path, str]</code> <p>when debug is enabled in <code>run()</code>, where to save the debug data.</p> <code>Path('.canals_debug/')</code> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def __init__(\n    self,\n    metadata: Optional[Dict[str, Any]] = None,\n    max_loops_allowed: int = 100,\n    debug_path: Union[Path, str] = Path(\".canals_debug/\"),\n):\n    \"\"\"\n    Creates the Pipeline.\n\n    Args:\n        metadata: arbitrary dictionary to store metadata about this pipeline. Make sure all the values contained in\n            this dictionary can be serialized and deserialized if you wish to save this pipeline to file with\n            `save_pipelines()/load_pipelines()`.\n        max_loops_allowed: how many times the pipeline can run the same node before throwing an exception.\n        debug_path: when debug is enabled in `run()`, where to save the debug data.\n    \"\"\"\n    self.metadata = metadata or {}\n    self.max_loops_allowed = max_loops_allowed\n    self.graph = networkx.MultiDiGraph()\n    self._connections: List[Connection] = []\n    self._mandatory_connections: Dict[str, List[Connection]] = defaultdict(list)\n    self._debug: Dict[int, Dict[str, Any]] = {}\n    self._debug_path = Path(debug_path)\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.add_component","title":"<code>add_component(name, instance)</code>","text":"<p>Create a component for the given component. Components are not connected to anything by default: use <code>Pipeline.connect()</code> to connect components together.</p> <p>Component names must be unique, but component instances can be reused if needed.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the component.</p> required <code>instance</code> <code>Component</code> <p>the component instance.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if a component with the same name already exists</p> <code>PipelineValidationError</code> <p>if the given instance is not a Canals component</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def add_component(self, name: str, instance: Component) -&gt; None:\n    \"\"\"\n    Create a component for the given component. Components are not connected to anything by default:\n    use `Pipeline.connect()` to connect components together.\n\n    Component names must be unique, but component instances can be reused if needed.\n\n    Args:\n        name: the name of the component.\n        instance: the component instance.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: if a component with the same name already exists\n        PipelineValidationError: if the given instance is not a Canals component\n    \"\"\"\n    # Component names are unique\n    if name in self.graph.nodes:\n        raise ValueError(f\"A component named '{name}' already exists in this pipeline: choose another name.\")\n\n    # Components can't be named `_debug`\n    if name == \"_debug\":\n        raise ValueError(\"'_debug' is a reserved name for debug output. Choose another name.\")\n\n    # Component instances must be components\n    if not isinstance(instance, Component):\n        raise PipelineValidationError(\n            f\"'{type(instance)}' doesn't seem to be a component. Is this class decorated with @component?\"\n        )\n\n    # Create the component's input and output sockets\n    input_sockets = getattr(instance, \"__canals_input__\", {})\n    output_sockets = getattr(instance, \"__canals_output__\", {})\n\n    # Add component to the graph, disconnected\n    logger.debug(\"Adding component '%s' (%s)\", name, instance)\n    self.graph.add_node(\n        name, instance=instance, input_sockets=input_sockets, output_sockets=output_sockets, visits=0\n    )\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.connect","title":"<code>connect(connect_from, connect_to)</code>","text":"<p>Connects two components together. All components to connect must exist in the pipeline. If connecting to an component that has several output connections, specify the inputs and output names as 'component_name.connections_name'.</p> <p>Parameters:</p> Name Type Description Default <code>connect_from</code> <code>str</code> <p>the component that delivers the value. This can be either just a component name or can be in the format <code>component_name.connection_name</code> if the component has multiple outputs.</p> required <code>connect_to</code> <code>str</code> <p>the component that receives the value. This can be either just a component name or can be in the format <code>component_name.connection_name</code> if the component has multiple inputs.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>PipelineConnectError</code> <p>if the two components cannot be connected (for example if one of the components is not present in the pipeline, or the connections don't match by type, and so on).</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def connect(self, connect_from: str, connect_to: str) -&gt; None:\n    \"\"\"\n    Connects two components together. All components to connect must exist in the pipeline.\n    If connecting to an component that has several output connections, specify the inputs and output names as\n    'component_name.connections_name'.\n\n    Args:\n        connect_from: the component that delivers the value. This can be either just a component name or can be\n            in the format `component_name.connection_name` if the component has multiple outputs.\n        connect_to: the component that receives the value. This can be either just a component name or can be\n            in the format `component_name.connection_name` if the component has multiple inputs.\n\n    Returns:\n        None\n\n    Raises:\n        PipelineConnectError: if the two components cannot be connected (for example if one of the components is\n            not present in the pipeline, or the connections don't match by type, and so on).\n    \"\"\"\n    # Edges may be named explicitly by passing 'node_name.edge_name' to connect().\n    sender, sender_socket_name = parse_connect_string(connect_from)\n    receiver, receiver_socket_name = parse_connect_string(connect_to)\n\n    # Get the nodes data.\n    try:\n        from_sockets = self.graph.nodes[sender][\"output_sockets\"]\n    except KeyError as exc:\n        raise ValueError(f\"Component named {sender} not found in the pipeline.\") from exc\n    try:\n        to_sockets = self.graph.nodes[receiver][\"input_sockets\"]\n    except KeyError as exc:\n        raise ValueError(f\"Component named {receiver} not found in the pipeline.\") from exc\n\n    # If the name of either socket is given, get the socket\n    sender_socket: Optional[OutputSocket] = None\n    if sender_socket_name:\n        sender_socket = from_sockets.get(sender_socket_name)\n        if not sender_socket:\n            raise PipelineConnectError(\n                f\"'{connect_from} does not exist. \"\n                f\"Output connections of {sender} are: \"\n                + \", \".join([f\"{name} (type {_type_name(socket.type)})\" for name, socket in from_sockets.items()])\n            )\n\n    receiver_socket: Optional[InputSocket] = None\n    if receiver_socket_name:\n        receiver_socket = to_sockets.get(receiver_socket_name)\n        if not receiver_socket:\n            raise PipelineConnectError(\n                f\"'{connect_to} does not exist. \"\n                f\"Input connections of {receiver} are: \"\n                + \", \".join([f\"{name} (type {_type_name(socket.type)})\" for name, socket in to_sockets.items()])\n            )\n\n    # Look for a matching connection among the possible ones.\n    # Note that if there is more than one possible connection but two sockets match by name, they're paired.\n    sender_socket_candidates: List[OutputSocket] = [sender_socket] if sender_socket else list(from_sockets.values())\n    receiver_socket_candidates: List[InputSocket] = (\n        [receiver_socket] if receiver_socket else list(to_sockets.values())\n    )\n\n    connection = Connection.from_list_of_sockets(\n        sender, sender_socket_candidates, receiver, receiver_socket_candidates\n    )\n\n    # Connection must be valid on both sender/receiver sides\n    if (\n        not connection.sender_socket\n        or not connection.receiver_socket\n        or not connection.sender\n        or not connection.receiver\n    ):\n        raise PipelineConnectError(\"Connection must have both sender and receiver: {connection}\")\n\n    # Create the connection\n    logger.debug(\n        \"Connecting '%s.%s' to '%s.%s'\",\n        connection.sender,\n        connection.sender_socket.name,\n        connection.receiver,\n        connection.receiver_socket.name,\n    )\n\n    self.graph.add_edge(\n        connection.sender,\n        connection.receiver,\n        key=f\"{connection.sender_socket.name}/{connection.receiver_socket.name}\",\n        conn_type=_type_name(connection.sender_socket.type),\n        from_socket=connection.sender_socket,\n        to_socket=connection.receiver_socket,\n    )\n\n    self._connections.append(connection)\n    if connection.is_mandatory:\n        self._mandatory_connections[connection.receiver].append(connection)\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.draw","title":"<code>draw(path, engine='mermaid-image')</code>","text":"<p>Draws the pipeline. Requires either <code>graphviz</code> as a system dependency, or an internet connection for Mermaid. Run <code>pip install canals[graphviz]</code> or <code>pip install canals[mermaid]</code> to install missing dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>where to save the diagram.</p> required <code>engine</code> <code>RenderingEngines</code> <p>which format to save the graph as. Accepts 'graphviz', 'mermaid-text', 'mermaid-image'. Default is 'mermaid-image'.</p> <code>'mermaid-image'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>if <code>engine='graphviz'</code> and <code>pygraphviz</code> is not installed.</p> <code>HTTPConnectionError</code> <p>(and similar) if the internet connection is down or other connection issues.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def draw(self, path: Path, engine: RenderingEngines = \"mermaid-image\") -&gt; None:\n    \"\"\"\n    Draws the pipeline. Requires either `graphviz` as a system dependency, or an internet connection for Mermaid.\n    Run `pip install canals[graphviz]` or `pip install canals[mermaid]` to install missing dependencies.\n\n    Args:\n        path: where to save the diagram.\n        engine: which format to save the graph as. Accepts 'graphviz', 'mermaid-text', 'mermaid-image'.\n            Default is 'mermaid-image'.\n\n    Returns:\n        None\n\n    Raises:\n        ImportError: if `engine='graphviz'` and `pygraphviz` is not installed.\n        HTTPConnectionError: (and similar) if the internet connection is down or other connection issues.\n    \"\"\"\n    _draw(graph=networkx.MultiDiGraph(self.graph), path=path, engine=engine)\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.from_dict","title":"<code>from_dict(data, **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates a Pipeline instance from a dictionary. A sample <code>data</code> dictionary could be formatted like so: <pre><code>{\n    \"metadata\": {\"test\": \"test\"},\n    \"max_loops_allowed\": 100,\n    \"components\": {\n        \"add_two\": {\n            \"type\": \"AddFixedValue\",\n            \"init_parameters\": {\"add\": 2},\n        },\n        \"add_default\": {\n            \"type\": \"AddFixedValue\",\n            \"init_parameters\": {\"add\": 1},\n        },\n        \"double\": {\n            \"type\": \"Double\",\n        },\n    },\n    \"connections\": [\n        {\"sender\": \"add_two.result\", \"receiver\": \"double.value\"},\n        {\"sender\": \"double.value\", \"receiver\": \"add_default.value\"},\n    ],\n}\n</code></pre></p> <p>Supported kwargs: <code>components</code>: a dictionary of {name: instance} to reuse instances of components instead of creating new ones.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef from_dict(cls: Type[T], data: Dict[str, Any], **kwargs) -&gt; T:\n    \"\"\"\n    Creates a Pipeline instance from a dictionary.\n    A sample `data` dictionary could be formatted like so:\n    ```\n    {\n        \"metadata\": {\"test\": \"test\"},\n        \"max_loops_allowed\": 100,\n        \"components\": {\n            \"add_two\": {\n                \"type\": \"AddFixedValue\",\n                \"init_parameters\": {\"add\": 2},\n            },\n            \"add_default\": {\n                \"type\": \"AddFixedValue\",\n                \"init_parameters\": {\"add\": 1},\n            },\n            \"double\": {\n                \"type\": \"Double\",\n            },\n        },\n        \"connections\": [\n            {\"sender\": \"add_two.result\", \"receiver\": \"double.value\"},\n            {\"sender\": \"double.value\", \"receiver\": \"add_default.value\"},\n        ],\n    }\n    ```\n\n    Supported kwargs:\n    `components`: a dictionary of {name: instance} to reuse instances of components instead of creating new ones.\n    \"\"\"\n    metadata = data.get(\"metadata\", {})\n    max_loops_allowed = data.get(\"max_loops_allowed\", 100)\n    debug_path = Path(data.get(\"debug_path\", \".canals_debug/\"))\n    pipe = cls(metadata=metadata, max_loops_allowed=max_loops_allowed, debug_path=debug_path)\n    components_to_reuse = kwargs.get(\"components\", {})\n    for name, component_data in data.get(\"components\", {}).items():\n        if name in components_to_reuse:\n            # Reuse an instance\n            instance = components_to_reuse[name]\n        else:\n            if \"type\" not in component_data:\n                raise PipelineError(f\"Missing 'type' in component '{name}'\")\n\n            if component_data[\"type\"] not in component.registry:\n                try:\n                    # Import the module first...\n                    module, _ = component_data[\"type\"].rsplit(\".\", 1)\n                    logger.debug(\"Trying to import %s\", module)\n                    importlib.import_module(module)\n                    # ...then try again\n                    if component_data[\"type\"] not in component.registry:\n                        raise PipelineError(\n                            f\"Successfully imported module {module} but can't find it in the component registry.\"\n                            \"This is unexpected and most likely a bug.\"\n                        )\n                except (ImportError, PipelineError) as e:\n                    raise PipelineError(f\"Component '{component_data['type']}' not imported.\") from e\n\n            # Create a new one\n            component_class = component.registry[component_data[\"type\"]]\n            instance = component_from_dict(component_class, component_data)\n        pipe.add_component(name=name, instance=instance)\n\n    for connection in data.get(\"connections\", []):\n        if \"sender\" not in connection:\n            raise PipelineError(f\"Missing sender in connection: {connection}\")\n        if \"receiver\" not in connection:\n            raise PipelineError(f\"Missing receiver in connection: {connection}\")\n        pipe.connect(connect_from=connection[\"sender\"], connect_to=connection[\"receiver\"])\n\n    return pipe\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.get_component","title":"<code>get_component(name)</code>","text":"<p>Returns an instance of a component.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the component</p> required <p>Returns:</p> Type Description <code>Component</code> <p>The instance of that component.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if a component with that name is not present in the pipeline.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def get_component(self, name: str) -&gt; Component:\n    \"\"\"\n    Returns an instance of a component.\n\n    Args:\n        name: the name of the component\n\n    Returns:\n        The instance of that component.\n\n    Raises:\n        ValueError: if a component with that name is not present in the pipeline.\n    \"\"\"\n    try:\n        return self.graph.nodes[name][\"instance\"]\n    except KeyError as exc:\n        raise ValueError(f\"Component named {name} not found in the pipeline.\") from exc\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.inputs","title":"<code>inputs()</code>","text":"<p>Returns a dictionary containing the inputs of a pipeline. Each key in the dictionary corresponds to a component name, and its value is another dictionary that describes the input sockets of that component, including their types and whether they are optional.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary where each key is a pipeline component name and each value is a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>inputs sockets of that component.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def inputs(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Returns a dictionary containing the inputs of a pipeline. Each key in the dictionary\n    corresponds to a component name, and its value is another dictionary that describes the\n    input sockets of that component, including their types and whether they are optional.\n\n    Returns:\n        A dictionary where each key is a pipeline component name and each value is a dictionary of\n        inputs sockets of that component.\n    \"\"\"\n    inputs = {\n        comp: {socket.name: {\"type\": socket.type, \"is_mandatory\": socket.is_mandatory} for socket in data}\n        for comp, data in find_pipeline_inputs(self.graph).items()\n        if data\n    }\n    return inputs\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.outputs","title":"<code>outputs()</code>","text":"<p>Returns a dictionary containing the outputs of a pipeline. Each key in the dictionary corresponds to a component name, and its value is another dictionary that describes the output sockets of that component.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary where each key is a pipeline component name and each value is a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>output sockets of that component.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def outputs(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Returns a dictionary containing the outputs of a pipeline. Each key in the dictionary\n    corresponds to a component name, and its value is another dictionary that describes the\n    output sockets of that component.\n\n    Returns:\n        A dictionary where each key is a pipeline component name and each value is a dictionary of\n        output sockets of that component.\n    \"\"\"\n    outputs = {\n        comp: {socket.name: {\"type\": socket.type} for socket in data}\n        for comp, data in find_pipeline_outputs(self.graph).items()\n        if data\n    }\n    return outputs\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.run","title":"<code>run(data, debug=False)</code>","text":"<p>Runs the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>the inputs to give to the input components of the Pipeline.</p> required <code>debug</code> <code>bool</code> <p>whether to collect and return debug information.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary with the outputs of the output components of the Pipeline.</p> <p>Raises:</p> Type Description <code>PipelineRuntimeError</code> <p>if the any of the components fail or return unexpected output.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def run(self, data: Dict[str, Any], debug: bool = False) -&gt; Dict[str, Any]:  # pylint: disable=too-many-locals\n    \"\"\"\n    Runs the pipeline.\n\n    Args:\n        data: the inputs to give to the input components of the Pipeline.\n        debug: whether to collect and return debug information.\n\n    Returns:\n        A dictionary with the outputs of the output components of the Pipeline.\n\n    Raises:\n        PipelineRuntimeError: if the any of the components fail or return unexpected output.\n    \"\"\"\n    self._clear_visits_count()\n    data = validate_pipeline_input(self.graph, input_values=data)\n    logger.info(\"Pipeline execution started.\")\n\n    self._debug = {}\n    if debug:\n        logger.info(\"Debug mode ON.\")\n        os.makedirs(\"debug\", exist_ok=True)\n\n    logger.debug(\n        \"Mandatory connections:\\n%s\",\n        \"\\n\".join(\n            f\" - {component}: {', '.join([str(s) for s in sockets])}\"\n            for component, sockets in self._mandatory_connections.items()\n        ),\n    )\n\n    self.warm_up()\n\n    # Prepare the inputs buffers and components queue\n    components_queue: List[str] = []\n    mandatory_values_buffer: Dict[Connection, Any] = {}\n    optional_values_buffer: Dict[Connection, Any] = {}\n    pipeline_output: Dict[str, Dict[str, Any]] = defaultdict(dict)\n\n    for node_name, input_data in data.items():\n        for socket_name, value in input_data.items():\n            # Make a copy of the input value so components don't need to\n            # take care of mutability.\n            value = deepcopy(value)\n            connection = Connection(\n                None, None, node_name, self.graph.nodes[node_name][\"input_sockets\"][socket_name]\n            )\n            self._add_value_to_buffers(\n                value, connection, components_queue, mandatory_values_buffer, optional_values_buffer\n            )\n\n    # *** PIPELINE EXECUTION LOOP ***\n    step = 0\n    while components_queue:  # pylint: disable=too-many-nested-blocks\n        step += 1\n        if debug:\n            self._record_pipeline_step(\n                step, components_queue, mandatory_values_buffer, optional_values_buffer, pipeline_output\n            )\n\n        component_name = components_queue.pop(0)\n        logger.debug(\"&gt; Queue at step %s: %s %s\", step, component_name, components_queue)\n        self._check_max_loops(component_name)\n\n        # **** RUN THE NODE ****\n        if not self._ready_to_run(component_name, mandatory_values_buffer, components_queue):\n            continue\n\n        inputs = {\n            **self._extract_inputs_from_buffer(component_name, mandatory_values_buffer),\n            **self._extract_inputs_from_buffer(component_name, optional_values_buffer),\n        }\n        outputs = self._run_component(name=component_name, inputs=dict(inputs))\n\n        # **** PROCESS THE OUTPUT ****\n        for socket_name, value in outputs.items():\n            targets = self._collect_targets(component_name, socket_name)\n            if not targets:\n                pipeline_output[component_name][socket_name] = value\n            else:\n                for target in targets:\n                    self._add_value_to_buffers(\n                        value, target, components_queue, mandatory_values_buffer, optional_values_buffer\n                    )\n\n    if debug:\n        self._record_pipeline_step(\n            step + 1, components_queue, mandatory_values_buffer, optional_values_buffer, pipeline_output\n        )\n        os.makedirs(self._debug_path, exist_ok=True)\n        with open(self._debug_path / \"data.json\", \"w\", encoding=\"utf-8\") as datafile:\n            json.dump(self._debug, datafile, indent=4, default=str)\n        pipeline_output[\"_debug\"] = self._debug  # type: ignore\n\n    logger.info(\"Pipeline executed successfully.\")\n    return dict(pipeline_output)\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.to_dict","title":"<code>to_dict()</code>","text":"<p>Returns this Pipeline instance as a dictionary. This is meant to be an intermediate representation but it can be also used to save a pipeline to file.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns this Pipeline instance as a dictionary.\n    This is meant to be an intermediate representation but it can be also used to save a pipeline to file.\n    \"\"\"\n    components = {}\n    for name, instance in self.graph.nodes(data=\"instance\"):  # type:ignore\n        components[name] = component_to_dict(instance)\n\n    connections = []\n    for sender, receiver, edge_data in self.graph.edges.data():\n        sender_socket = edge_data[\"from_socket\"].name\n        receiver_socket = edge_data[\"to_socket\"].name\n        connections.append({\"sender\": f\"{sender}.{sender_socket}\", \"receiver\": f\"{receiver}.{receiver_socket}\"})\n    return {\n        \"metadata\": self.metadata,\n        \"max_loops_allowed\": self.max_loops_allowed,\n        \"components\": components,\n        \"connections\": connections,\n    }\n</code></pre>"},{"location":"api-docs/canals/#canals.Pipeline.warm_up","title":"<code>warm_up()</code>","text":"<p>Make sure all nodes are warm.</p> <p>It's the node's responsibility to make sure this method can be called at every <code>Pipeline.run()</code> without re-initializing everything.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def warm_up(self):\n    \"\"\"\n    Make sure all nodes are warm.\n\n    It's the node's responsibility to make sure this method can be called at every `Pipeline.run()`\n    without re-initializing everything.\n    \"\"\"\n    for node in self.graph.nodes:\n        if hasattr(self.graph.nodes[node][\"instance\"], \"warm_up\"):\n            logger.info(\"Warming up component %s...\", node)\n            self.graph.nodes[node][\"instance\"].warm_up()\n</code></pre>"},{"location":"api-docs/canals/#canals.serialization.component_from_dict","title":"<code>component_from_dict(cls, data)</code>","text":"<p>The unmarshaller used by the Pipeline. If a <code>from_dict</code> method is present in the component instance, that will be used instead of the default method.</p> Source code in <code>canals/serialization.py</code> <pre><code>def component_from_dict(cls: Type[object], data: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    The unmarshaller used by the Pipeline. If a `from_dict` method is present in the\n    component instance, that will be used instead of the default method.\n    \"\"\"\n    if hasattr(cls, \"from_dict\"):\n        return cls.from_dict(data)\n\n    return default_from_dict(cls, data)\n</code></pre>"},{"location":"api-docs/canals/#canals.serialization.component_to_dict","title":"<code>component_to_dict(obj)</code>","text":"<p>The marshaller used by the Pipeline. If a <code>to_dict</code> method is present in the component instance, that will be used instead of the default method.</p> Source code in <code>canals/serialization.py</code> <pre><code>def component_to_dict(obj: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    The marshaller used by the Pipeline. If a `to_dict` method is present in the\n    component instance, that will be used instead of the default method.\n    \"\"\"\n    if hasattr(obj, \"to_dict\"):\n        return obj.to_dict()\n\n    init_parameters = {}\n    for name, param in inspect.signature(obj.__init__).parameters.items():\n        # Ignore `args` and `kwargs`, used by the default constructor\n        if name in (\"args\", \"kwargs\"):\n            continue\n        try:\n            # This only works if the Component constructor assigns the init\n            # parameter to an instance variable or property with the same name\n            param_value = getattr(obj, name)\n        except AttributeError as e:\n            # If the parameter doesn't have a default value, raise an error\n            if param.default is param.empty:\n                raise SerializationError(\n                    f\"Cannot determine the value of the init parameter '{name}' for the class {obj.__class__.__name__}.\"\n                    f\"You can fix this error by assigning 'self.{name} = {name}' or adding a \"\n                    f\"custom serialization method 'to_dict' to the class.\"\n                ) from e\n            # In case the init parameter was not assigned, we use the default value\n            param_value = param.default\n        init_parameters[name] = param_value\n\n    return default_to_dict(obj, **init_parameters)\n</code></pre>"},{"location":"api-docs/canals/#canals.serialization.default_from_dict","title":"<code>default_from_dict(cls, data)</code>","text":"<p>Utility function to deserialize a dictionary to an object. This is mostly necessary for Components but it can be used by any object.</p> <p>The function will raise a <code>DeserializationError</code> if the <code>type</code> field in <code>data</code> is missing or it doesn't match the type of <code>cls</code>.</p> <p>If <code>data</code> contains an <code>init_parameters</code> field it will be used as parameters to create a new instance of <code>cls</code>.</p> Source code in <code>canals/serialization.py</code> <pre><code>def default_from_dict(cls: Type[object], data: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Utility function to deserialize a dictionary to an object.\n    This is mostly necessary for Components but it can be used by any object.\n\n    The function will raise a `DeserializationError` if the `type` field in `data` is\n    missing or it doesn't match the type of `cls`.\n\n    If `data` contains an `init_parameters` field it will be used as parameters to create\n    a new instance of `cls`.\n    \"\"\"\n    init_params = data.get(\"init_parameters\", {})\n    if \"type\" not in data:\n        raise DeserializationError(\"Missing 'type' in serialization data\")\n    if data[\"type\"] != f\"{cls.__module__}.{cls.__name__}\":\n        raise DeserializationError(f\"Class '{data['type']}' can't be deserialized as '{cls.__name__}'\")\n    return cls(**init_params)\n</code></pre>"},{"location":"api-docs/canals/#canals.serialization.default_to_dict","title":"<code>default_to_dict(obj, **init_parameters)</code>","text":"<p>Utility function to serialize an object to a dictionary. This is mostly necessary for Components but it can be used by any object.</p> <p><code>init_parameters</code> are parameters passed to the object class <code>__init__</code>. They must be defined explicitly as they'll be used when creating a new instance of <code>obj</code> with <code>from_dict</code>. Omitting them might cause deserialisation errors or unexpected behaviours later, when calling <code>from_dict</code>.</p> <p>An example usage:</p> <pre><code>class MyClass:\n    def __init__(self, my_param: int = 10):\n        self.my_param = my_param\n\n    def to_dict(self):\n        return default_to_dict(self, my_param=self.my_param)\n\n\nobj = MyClass(my_param=1000)\ndata = obj.to_dict()\nassert data == {\n    \"type\": \"MyClass\",\n    \"init_parameters\": {\n        \"my_param\": 1000,\n    },\n}\n</code></pre> Source code in <code>canals/serialization.py</code> <pre><code>def default_to_dict(obj: Any, **init_parameters) -&gt; Dict[str, Any]:\n    \"\"\"\n    Utility function to serialize an object to a dictionary.\n    This is mostly necessary for Components but it can be used by any object.\n\n    `init_parameters` are parameters passed to the object class `__init__`.\n    They must be defined explicitly as they'll be used when creating a new\n    instance of `obj` with `from_dict`. Omitting them might cause deserialisation\n    errors or unexpected behaviours later, when calling `from_dict`.\n\n    An example usage:\n\n    ```python\n    class MyClass:\n        def __init__(self, my_param: int = 10):\n            self.my_param = my_param\n\n        def to_dict(self):\n            return default_to_dict(self, my_param=self.my_param)\n\n\n    obj = MyClass(my_param=1000)\n    data = obj.to_dict()\n    assert data == {\n        \"type\": \"MyClass\",\n        \"init_parameters\": {\n            \"my_param\": 1000,\n        },\n    }\n    ```\n    \"\"\"\n    return {\n        \"type\": f\"{obj.__class__.__module__}.{obj.__class__.__name__}\",\n        \"init_parameters\": init_parameters,\n    }\n</code></pre>"},{"location":"api-docs/component/","title":"Component API","text":"<p>Attributes:</p> <pre><code>component: Marks a class as a component. Any class decorated with `@component` can be used by a Pipeline.\n</code></pre> <p>All components must follow the contract below. This docstring is the source of truth for components contract.</p> <p><code>@component</code> decorator</p> <p>All component classes must be decorated with the <code>@component</code> decorator. This allows Canals to discover them.</p> <p><code>__init__(self, **kwargs)</code></p> <p>Optional method.</p> <p>Components may have an <code>__init__</code> method where they define:</p> <ul> <li><code>self.init_parameters = {same parameters that the __init__ method received}</code>:     In this dictionary you can store any state the components wish to be persisted when they are saved.     These values will be given to the <code>__init__</code> method of a new instance when the pipeline is loaded.     Note that by default the <code>@component</code> decorator saves the arguments automatically.     However, if a component sets their own <code>init_parameters</code> manually in <code>__init__()</code>, that will be used instead.     Note: all of the values contained here must be JSON serializable. Serialize them manually if needed.</li> </ul> <p>Components should take only \"basic\" Python types as parameters of their <code>__init__</code> function, or iterables and dictionaries containing only such values. Anything else (objects, functions, etc) will raise an exception at init time. If there's the need for such values, consider serializing them to a string.</p> <p>(TODO explain how to use classes and functions in init. In the meantime see <code>test/components/test_accumulate.py</code>)</p> <p>The <code>__init__</code> must be extrememly lightweight, because it's a frequent operation during the construction and validation of the pipeline. If a component has some heavy state to initialize (models, backends, etc...) refer to the <code>warm_up()</code> method.</p> <p><code>warm_up(self)</code></p> <p>Optional method.</p> <p>This method is called by Pipeline before the graph execution. Make sure to avoid double-initializations, because Pipeline will not keep track of which components it called <code>warm_up()</code> on.</p> <p><code>run(self, data)</code></p> <p>Mandatory method.</p> <p>This is the method where the main functionality of the component should be carried out. It's called by <code>Pipeline.run()</code>.</p> <p>When the component should run, Pipeline will call this method with an instance of the dataclass returned by the method decorated with <code>@component.input</code>. This dataclass contains:</p> <ul> <li>all the input values coming from other components connected to it,</li> <li>if any is missing, the corresponding value defined in <code>self.defaults</code>, if it exists.</li> </ul> <p><code>run()</code> must return a single instance of the dataclass declared through the method decorated with <code>@component.output</code>.</p>"},{"location":"api-docs/component/#canals.component.Component","title":"<code>Component</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Note this is only used by type checking tools.</p> <p>In order to implement the <code>Component</code> protocol, custom components need to have a <code>run</code> method. The signature of the method and its return value won't be checked, i.e. classes with the following methods:</p> <pre><code>def run(self, param: str) -&gt; Dict[str, Any]:\n    ...\n</code></pre> <p>and</p> <pre><code>def run(self, **kwargs):\n    ...\n</code></pre> <p>will be both considered as respecting the protocol. This makes the type checking much weaker, but we have other places where we ensure code is dealing with actual Components.</p> <p>The protocol is runtime checkable so it'll be possible to assert:</p> <pre><code>isinstance(MyComponent, Component)\n</code></pre> Source code in <code>canals/component/component.py</code> <pre><code>@runtime_checkable\nclass Component(Protocol):\n    \"\"\"\n    Note this is only used by type checking tools.\n\n    In order to implement the `Component` protocol, custom components need to\n    have a `run` method. The signature of the method and its return value\n    won't be checked, i.e. classes with the following methods:\n\n        def run(self, param: str) -&gt; Dict[str, Any]:\n            ...\n\n    and\n\n        def run(self, **kwargs):\n            ...\n\n    will be both considered as respecting the protocol. This makes the type\n    checking much weaker, but we have other places where we ensure code is\n    dealing with actual Components.\n\n    The protocol is runtime checkable so it'll be possible to assert:\n\n        isinstance(MyComponent, Component)\n    \"\"\"\n\n    def run(self, *args: Any, **kwargs: Any):  # pylint: disable=missing-function-docstring\n        ...\n</code></pre>"},{"location":"api-docs/component/#canals.component.component.Component","title":"<code>Component</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Note this is only used by type checking tools.</p> <p>In order to implement the <code>Component</code> protocol, custom components need to have a <code>run</code> method. The signature of the method and its return value won't be checked, i.e. classes with the following methods:</p> <pre><code>def run(self, param: str) -&gt; Dict[str, Any]:\n    ...\n</code></pre> <p>and</p> <pre><code>def run(self, **kwargs):\n    ...\n</code></pre> <p>will be both considered as respecting the protocol. This makes the type checking much weaker, but we have other places where we ensure code is dealing with actual Components.</p> <p>The protocol is runtime checkable so it'll be possible to assert:</p> <pre><code>isinstance(MyComponent, Component)\n</code></pre> Source code in <code>canals/component/component.py</code> <pre><code>@runtime_checkable\nclass Component(Protocol):\n    \"\"\"\n    Note this is only used by type checking tools.\n\n    In order to implement the `Component` protocol, custom components need to\n    have a `run` method. The signature of the method and its return value\n    won't be checked, i.e. classes with the following methods:\n\n        def run(self, param: str) -&gt; Dict[str, Any]:\n            ...\n\n    and\n\n        def run(self, **kwargs):\n            ...\n\n    will be both considered as respecting the protocol. This makes the type\n    checking much weaker, but we have other places where we ensure code is\n    dealing with actual Components.\n\n    The protocol is runtime checkable so it'll be possible to assert:\n\n        isinstance(MyComponent, Component)\n    \"\"\"\n\n    def run(self, *args: Any, **kwargs: Any):  # pylint: disable=missing-function-docstring\n        ...\n</code></pre>"},{"location":"api-docs/component/#canals.component.component.ComponentMeta","title":"<code>ComponentMeta</code>","text":"<p>             Bases: <code>type</code></p> Source code in <code>canals/component/component.py</code> <pre><code>class ComponentMeta(type):\n    def __call__(cls, *args, **kwargs):\n        \"\"\"\n        This method is called when clients instantiate a Component and\n        runs before __new__ and __init__.\n        \"\"\"\n        # This will call __new__ then __init__, giving us back the Component instance\n        instance = super().__call__(*args, **kwargs)\n\n        # Before returning, we have the chance to modify the newly created\n        # Component instance, so we take the chance and set up the I/O sockets\n\n        # If `component.set_output_types()` was called in the component constructor,\n        # `__canals_output__` is already populated, no need to do anything.\n        if not hasattr(instance, \"__canals_output__\"):\n            # If that's not the case, we need to populate `__canals_output__`\n            #\n            # If the `run` method was decorated, it has a `_output_types_cache` field assigned\n            # that stores the output specification.\n            # We deepcopy the content of the cache to transfer ownership from the class method\n            # to the actual instance, so that different instances of the same class won't share this data.\n            instance.__canals_output__ = deepcopy(getattr(instance.run, \"_output_types_cache\", {}))\n\n        # Create the sockets if set_input_types() wasn't called in the constructor.\n        # If it was called and there are some parameters also in the `run()` method, these take precedence.\n        if not hasattr(instance, \"__canals_input__\"):\n            instance.__canals_input__ = {}\n        run_signature = inspect.signature(getattr(cls, \"run\"))\n        for param in list(run_signature.parameters)[1:]:  # First is 'self' and it doesn't matter.\n            if run_signature.parameters[param].kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:  # ignore `**kwargs`\n                instance.__canals_input__[param] = InputSocket(\n                    name=param,\n                    type=run_signature.parameters[param].annotation,\n                    is_mandatory=run_signature.parameters[param].default == inspect.Parameter.empty,\n                )\n        return instance\n</code></pre>"},{"location":"api-docs/component/#canals.component.component.ComponentMeta.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>This method is called when clients instantiate a Component and runs before new and init.</p> Source code in <code>canals/component/component.py</code> <pre><code>def __call__(cls, *args, **kwargs):\n    \"\"\"\n    This method is called when clients instantiate a Component and\n    runs before __new__ and __init__.\n    \"\"\"\n    # This will call __new__ then __init__, giving us back the Component instance\n    instance = super().__call__(*args, **kwargs)\n\n    # Before returning, we have the chance to modify the newly created\n    # Component instance, so we take the chance and set up the I/O sockets\n\n    # If `component.set_output_types()` was called in the component constructor,\n    # `__canals_output__` is already populated, no need to do anything.\n    if not hasattr(instance, \"__canals_output__\"):\n        # If that's not the case, we need to populate `__canals_output__`\n        #\n        # If the `run` method was decorated, it has a `_output_types_cache` field assigned\n        # that stores the output specification.\n        # We deepcopy the content of the cache to transfer ownership from the class method\n        # to the actual instance, so that different instances of the same class won't share this data.\n        instance.__canals_output__ = deepcopy(getattr(instance.run, \"_output_types_cache\", {}))\n\n    # Create the sockets if set_input_types() wasn't called in the constructor.\n    # If it was called and there are some parameters also in the `run()` method, these take precedence.\n    if not hasattr(instance, \"__canals_input__\"):\n        instance.__canals_input__ = {}\n    run_signature = inspect.signature(getattr(cls, \"run\"))\n    for param in list(run_signature.parameters)[1:]:  # First is 'self' and it doesn't matter.\n        if run_signature.parameters[param].kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:  # ignore `**kwargs`\n            instance.__canals_input__[param] = InputSocket(\n                name=param,\n                type=run_signature.parameters[param].annotation,\n                is_mandatory=run_signature.parameters[param].default == inspect.Parameter.empty,\n            )\n    return instance\n</code></pre>"},{"location":"api-docs/pipeline/","title":"Pipeline","text":""},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>Components orchestration engine.</p> <p>Builds a graph of components and orchestrates their execution according to the execution graph.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"\n    Components orchestration engine.\n\n    Builds a graph of components and orchestrates their execution according to the execution graph.\n    \"\"\"\n\n    def __init__(\n        self,\n        metadata: Optional[Dict[str, Any]] = None,\n        max_loops_allowed: int = 100,\n        debug_path: Union[Path, str] = Path(\".canals_debug/\"),\n    ):\n        \"\"\"\n        Creates the Pipeline.\n\n        Args:\n            metadata: arbitrary dictionary to store metadata about this pipeline. Make sure all the values contained in\n                this dictionary can be serialized and deserialized if you wish to save this pipeline to file with\n                `save_pipelines()/load_pipelines()`.\n            max_loops_allowed: how many times the pipeline can run the same node before throwing an exception.\n            debug_path: when debug is enabled in `run()`, where to save the debug data.\n        \"\"\"\n        self.metadata = metadata or {}\n        self.max_loops_allowed = max_loops_allowed\n        self.graph = networkx.MultiDiGraph()\n        self._connections: List[Connection] = []\n        self._mandatory_connections: Dict[str, List[Connection]] = defaultdict(list)\n        self._debug: Dict[int, Dict[str, Any]] = {}\n        self._debug_path = Path(debug_path)\n\n    def __eq__(self, other) -&gt; bool:\n        \"\"\"\n        Equal pipelines share every metadata, node and edge, but they're not required to use\n        the same node instances: this allows pipeline saved and then loaded back to be equal to themselves.\n        \"\"\"\n        if (\n            not isinstance(other, type(self))\n            or not getattr(self, \"metadata\") == getattr(other, \"metadata\")\n            or not getattr(self, \"max_loops_allowed\") == getattr(other, \"max_loops_allowed\")\n            or not hasattr(self, \"graph\")\n            or not hasattr(other, \"graph\")\n        ):\n            return False\n\n        return (\n            self.graph.adj == other.graph.adj\n            and self._comparable_nodes_list(self.graph) == self._comparable_nodes_list(other.graph)\n            and self.graph.graph == other.graph.graph\n        )\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Returns this Pipeline instance as a dictionary.\n        This is meant to be an intermediate representation but it can be also used to save a pipeline to file.\n        \"\"\"\n        components = {}\n        for name, instance in self.graph.nodes(data=\"instance\"):  # type:ignore\n            components[name] = component_to_dict(instance)\n\n        connections = []\n        for sender, receiver, edge_data in self.graph.edges.data():\n            sender_socket = edge_data[\"from_socket\"].name\n            receiver_socket = edge_data[\"to_socket\"].name\n            connections.append({\"sender\": f\"{sender}.{sender_socket}\", \"receiver\": f\"{receiver}.{receiver_socket}\"})\n        return {\n            \"metadata\": self.metadata,\n            \"max_loops_allowed\": self.max_loops_allowed,\n            \"components\": components,\n            \"connections\": connections,\n        }\n\n    @classmethod\n    def from_dict(cls: Type[T], data: Dict[str, Any], **kwargs) -&gt; T:\n        \"\"\"\n        Creates a Pipeline instance from a dictionary.\n        A sample `data` dictionary could be formatted like so:\n        ```\n        {\n            \"metadata\": {\"test\": \"test\"},\n            \"max_loops_allowed\": 100,\n            \"components\": {\n                \"add_two\": {\n                    \"type\": \"AddFixedValue\",\n                    \"init_parameters\": {\"add\": 2},\n                },\n                \"add_default\": {\n                    \"type\": \"AddFixedValue\",\n                    \"init_parameters\": {\"add\": 1},\n                },\n                \"double\": {\n                    \"type\": \"Double\",\n                },\n            },\n            \"connections\": [\n                {\"sender\": \"add_two.result\", \"receiver\": \"double.value\"},\n                {\"sender\": \"double.value\", \"receiver\": \"add_default.value\"},\n            ],\n        }\n        ```\n\n        Supported kwargs:\n        `components`: a dictionary of {name: instance} to reuse instances of components instead of creating new ones.\n        \"\"\"\n        metadata = data.get(\"metadata\", {})\n        max_loops_allowed = data.get(\"max_loops_allowed\", 100)\n        debug_path = Path(data.get(\"debug_path\", \".canals_debug/\"))\n        pipe = cls(metadata=metadata, max_loops_allowed=max_loops_allowed, debug_path=debug_path)\n        components_to_reuse = kwargs.get(\"components\", {})\n        for name, component_data in data.get(\"components\", {}).items():\n            if name in components_to_reuse:\n                # Reuse an instance\n                instance = components_to_reuse[name]\n            else:\n                if \"type\" not in component_data:\n                    raise PipelineError(f\"Missing 'type' in component '{name}'\")\n\n                if component_data[\"type\"] not in component.registry:\n                    try:\n                        # Import the module first...\n                        module, _ = component_data[\"type\"].rsplit(\".\", 1)\n                        logger.debug(\"Trying to import %s\", module)\n                        importlib.import_module(module)\n                        # ...then try again\n                        if component_data[\"type\"] not in component.registry:\n                            raise PipelineError(\n                                f\"Successfully imported module {module} but can't find it in the component registry.\"\n                                \"This is unexpected and most likely a bug.\"\n                            )\n                    except (ImportError, PipelineError) as e:\n                        raise PipelineError(f\"Component '{component_data['type']}' not imported.\") from e\n\n                # Create a new one\n                component_class = component.registry[component_data[\"type\"]]\n                instance = component_from_dict(component_class, component_data)\n            pipe.add_component(name=name, instance=instance)\n\n        for connection in data.get(\"connections\", []):\n            if \"sender\" not in connection:\n                raise PipelineError(f\"Missing sender in connection: {connection}\")\n            if \"receiver\" not in connection:\n                raise PipelineError(f\"Missing receiver in connection: {connection}\")\n            pipe.connect(connect_from=connection[\"sender\"], connect_to=connection[\"receiver\"])\n\n        return pipe\n\n    def _comparable_nodes_list(self, graph: networkx.MultiDiGraph) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Replaces instances of nodes with their class name in order to make sure they're comparable.\n        \"\"\"\n        nodes = []\n        for node in graph.nodes:\n            comparable_node = graph.nodes[node]\n            comparable_node[\"instance\"] = comparable_node[\"instance\"].__class__\n            nodes.append(comparable_node)\n        nodes.sort()\n        return nodes\n\n    def add_component(self, name: str, instance: Component) -&gt; None:\n        \"\"\"\n        Create a component for the given component. Components are not connected to anything by default:\n        use `Pipeline.connect()` to connect components together.\n\n        Component names must be unique, but component instances can be reused if needed.\n\n        Args:\n            name: the name of the component.\n            instance: the component instance.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: if a component with the same name already exists\n            PipelineValidationError: if the given instance is not a Canals component\n        \"\"\"\n        # Component names are unique\n        if name in self.graph.nodes:\n            raise ValueError(f\"A component named '{name}' already exists in this pipeline: choose another name.\")\n\n        # Components can't be named `_debug`\n        if name == \"_debug\":\n            raise ValueError(\"'_debug' is a reserved name for debug output. Choose another name.\")\n\n        # Component instances must be components\n        if not isinstance(instance, Component):\n            raise PipelineValidationError(\n                f\"'{type(instance)}' doesn't seem to be a component. Is this class decorated with @component?\"\n            )\n\n        # Create the component's input and output sockets\n        input_sockets = getattr(instance, \"__canals_input__\", {})\n        output_sockets = getattr(instance, \"__canals_output__\", {})\n\n        # Add component to the graph, disconnected\n        logger.debug(\"Adding component '%s' (%s)\", name, instance)\n        self.graph.add_node(\n            name, instance=instance, input_sockets=input_sockets, output_sockets=output_sockets, visits=0\n        )\n\n    def connect(self, connect_from: str, connect_to: str) -&gt; None:\n        \"\"\"\n        Connects two components together. All components to connect must exist in the pipeline.\n        If connecting to an component that has several output connections, specify the inputs and output names as\n        'component_name.connections_name'.\n\n        Args:\n            connect_from: the component that delivers the value. This can be either just a component name or can be\n                in the format `component_name.connection_name` if the component has multiple outputs.\n            connect_to: the component that receives the value. This can be either just a component name or can be\n                in the format `component_name.connection_name` if the component has multiple inputs.\n\n        Returns:\n            None\n\n        Raises:\n            PipelineConnectError: if the two components cannot be connected (for example if one of the components is\n                not present in the pipeline, or the connections don't match by type, and so on).\n        \"\"\"\n        # Edges may be named explicitly by passing 'node_name.edge_name' to connect().\n        sender, sender_socket_name = parse_connect_string(connect_from)\n        receiver, receiver_socket_name = parse_connect_string(connect_to)\n\n        # Get the nodes data.\n        try:\n            from_sockets = self.graph.nodes[sender][\"output_sockets\"]\n        except KeyError as exc:\n            raise ValueError(f\"Component named {sender} not found in the pipeline.\") from exc\n        try:\n            to_sockets = self.graph.nodes[receiver][\"input_sockets\"]\n        except KeyError as exc:\n            raise ValueError(f\"Component named {receiver} not found in the pipeline.\") from exc\n\n        # If the name of either socket is given, get the socket\n        sender_socket: Optional[OutputSocket] = None\n        if sender_socket_name:\n            sender_socket = from_sockets.get(sender_socket_name)\n            if not sender_socket:\n                raise PipelineConnectError(\n                    f\"'{connect_from} does not exist. \"\n                    f\"Output connections of {sender} are: \"\n                    + \", \".join([f\"{name} (type {_type_name(socket.type)})\" for name, socket in from_sockets.items()])\n                )\n\n        receiver_socket: Optional[InputSocket] = None\n        if receiver_socket_name:\n            receiver_socket = to_sockets.get(receiver_socket_name)\n            if not receiver_socket:\n                raise PipelineConnectError(\n                    f\"'{connect_to} does not exist. \"\n                    f\"Input connections of {receiver} are: \"\n                    + \", \".join([f\"{name} (type {_type_name(socket.type)})\" for name, socket in to_sockets.items()])\n                )\n\n        # Look for a matching connection among the possible ones.\n        # Note that if there is more than one possible connection but two sockets match by name, they're paired.\n        sender_socket_candidates: List[OutputSocket] = [sender_socket] if sender_socket else list(from_sockets.values())\n        receiver_socket_candidates: List[InputSocket] = (\n            [receiver_socket] if receiver_socket else list(to_sockets.values())\n        )\n\n        connection = Connection.from_list_of_sockets(\n            sender, sender_socket_candidates, receiver, receiver_socket_candidates\n        )\n\n        # Connection must be valid on both sender/receiver sides\n        if (\n            not connection.sender_socket\n            or not connection.receiver_socket\n            or not connection.sender\n            or not connection.receiver\n        ):\n            raise PipelineConnectError(\"Connection must have both sender and receiver: {connection}\")\n\n        # Create the connection\n        logger.debug(\n            \"Connecting '%s.%s' to '%s.%s'\",\n            connection.sender,\n            connection.sender_socket.name,\n            connection.receiver,\n            connection.receiver_socket.name,\n        )\n\n        self.graph.add_edge(\n            connection.sender,\n            connection.receiver,\n            key=f\"{connection.sender_socket.name}/{connection.receiver_socket.name}\",\n            conn_type=_type_name(connection.sender_socket.type),\n            from_socket=connection.sender_socket,\n            to_socket=connection.receiver_socket,\n        )\n\n        self._connections.append(connection)\n        if connection.is_mandatory:\n            self._mandatory_connections[connection.receiver].append(connection)\n\n    def get_component(self, name: str) -&gt; Component:\n        \"\"\"\n        Returns an instance of a component.\n\n        Args:\n            name: the name of the component\n\n        Returns:\n            The instance of that component.\n\n        Raises:\n            ValueError: if a component with that name is not present in the pipeline.\n        \"\"\"\n        try:\n            return self.graph.nodes[name][\"instance\"]\n        except KeyError as exc:\n            raise ValueError(f\"Component named {name} not found in the pipeline.\") from exc\n\n    def inputs(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Returns a dictionary containing the inputs of a pipeline. Each key in the dictionary\n        corresponds to a component name, and its value is another dictionary that describes the\n        input sockets of that component, including their types and whether they are optional.\n\n        Returns:\n            A dictionary where each key is a pipeline component name and each value is a dictionary of\n            inputs sockets of that component.\n        \"\"\"\n        inputs = {\n            comp: {socket.name: {\"type\": socket.type, \"is_mandatory\": socket.is_mandatory} for socket in data}\n            for comp, data in find_pipeline_inputs(self.graph).items()\n            if data\n        }\n        return inputs\n\n    def outputs(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Returns a dictionary containing the outputs of a pipeline. Each key in the dictionary\n        corresponds to a component name, and its value is another dictionary that describes the\n        output sockets of that component.\n\n        Returns:\n            A dictionary where each key is a pipeline component name and each value is a dictionary of\n            output sockets of that component.\n        \"\"\"\n        outputs = {\n            comp: {socket.name: {\"type\": socket.type} for socket in data}\n            for comp, data in find_pipeline_outputs(self.graph).items()\n            if data\n        }\n        return outputs\n\n    def draw(self, path: Path, engine: RenderingEngines = \"mermaid-image\") -&gt; None:\n        \"\"\"\n        Draws the pipeline. Requires either `graphviz` as a system dependency, or an internet connection for Mermaid.\n        Run `pip install canals[graphviz]` or `pip install canals[mermaid]` to install missing dependencies.\n\n        Args:\n            path: where to save the diagram.\n            engine: which format to save the graph as. Accepts 'graphviz', 'mermaid-text', 'mermaid-image'.\n                Default is 'mermaid-image'.\n\n        Returns:\n            None\n\n        Raises:\n            ImportError: if `engine='graphviz'` and `pygraphviz` is not installed.\n            HTTPConnectionError: (and similar) if the internet connection is down or other connection issues.\n        \"\"\"\n        _draw(graph=networkx.MultiDiGraph(self.graph), path=path, engine=engine)\n\n    def warm_up(self):\n        \"\"\"\n        Make sure all nodes are warm.\n\n        It's the node's responsibility to make sure this method can be called at every `Pipeline.run()`\n        without re-initializing everything.\n        \"\"\"\n        for node in self.graph.nodes:\n            if hasattr(self.graph.nodes[node][\"instance\"], \"warm_up\"):\n                logger.info(\"Warming up component %s...\", node)\n                self.graph.nodes[node][\"instance\"].warm_up()\n\n    def run(self, data: Dict[str, Any], debug: bool = False) -&gt; Dict[str, Any]:  # pylint: disable=too-many-locals\n        \"\"\"\n        Runs the pipeline.\n\n        Args:\n            data: the inputs to give to the input components of the Pipeline.\n            debug: whether to collect and return debug information.\n\n        Returns:\n            A dictionary with the outputs of the output components of the Pipeline.\n\n        Raises:\n            PipelineRuntimeError: if the any of the components fail or return unexpected output.\n        \"\"\"\n        self._clear_visits_count()\n        data = validate_pipeline_input(self.graph, input_values=data)\n        logger.info(\"Pipeline execution started.\")\n\n        self._debug = {}\n        if debug:\n            logger.info(\"Debug mode ON.\")\n            os.makedirs(\"debug\", exist_ok=True)\n\n        logger.debug(\n            \"Mandatory connections:\\n%s\",\n            \"\\n\".join(\n                f\" - {component}: {', '.join([str(s) for s in sockets])}\"\n                for component, sockets in self._mandatory_connections.items()\n            ),\n        )\n\n        self.warm_up()\n\n        # Prepare the inputs buffers and components queue\n        components_queue: List[str] = []\n        mandatory_values_buffer: Dict[Connection, Any] = {}\n        optional_values_buffer: Dict[Connection, Any] = {}\n        pipeline_output: Dict[str, Dict[str, Any]] = defaultdict(dict)\n\n        for node_name, input_data in data.items():\n            for socket_name, value in input_data.items():\n                # Make a copy of the input value so components don't need to\n                # take care of mutability.\n                value = deepcopy(value)\n                connection = Connection(\n                    None, None, node_name, self.graph.nodes[node_name][\"input_sockets\"][socket_name]\n                )\n                self._add_value_to_buffers(\n                    value, connection, components_queue, mandatory_values_buffer, optional_values_buffer\n                )\n\n        # *** PIPELINE EXECUTION LOOP ***\n        step = 0\n        while components_queue:  # pylint: disable=too-many-nested-blocks\n            step += 1\n            if debug:\n                self._record_pipeline_step(\n                    step, components_queue, mandatory_values_buffer, optional_values_buffer, pipeline_output\n                )\n\n            component_name = components_queue.pop(0)\n            logger.debug(\"&gt; Queue at step %s: %s %s\", step, component_name, components_queue)\n            self._check_max_loops(component_name)\n\n            # **** RUN THE NODE ****\n            if not self._ready_to_run(component_name, mandatory_values_buffer, components_queue):\n                continue\n\n            inputs = {\n                **self._extract_inputs_from_buffer(component_name, mandatory_values_buffer),\n                **self._extract_inputs_from_buffer(component_name, optional_values_buffer),\n            }\n            outputs = self._run_component(name=component_name, inputs=dict(inputs))\n\n            # **** PROCESS THE OUTPUT ****\n            for socket_name, value in outputs.items():\n                targets = self._collect_targets(component_name, socket_name)\n                if not targets:\n                    pipeline_output[component_name][socket_name] = value\n                else:\n                    for target in targets:\n                        self._add_value_to_buffers(\n                            value, target, components_queue, mandatory_values_buffer, optional_values_buffer\n                        )\n\n        if debug:\n            self._record_pipeline_step(\n                step + 1, components_queue, mandatory_values_buffer, optional_values_buffer, pipeline_output\n            )\n            os.makedirs(self._debug_path, exist_ok=True)\n            with open(self._debug_path / \"data.json\", \"w\", encoding=\"utf-8\") as datafile:\n                json.dump(self._debug, datafile, indent=4, default=str)\n            pipeline_output[\"_debug\"] = self._debug  # type: ignore\n\n        logger.info(\"Pipeline executed successfully.\")\n        return dict(pipeline_output)\n\n    def _record_pipeline_step(\n        self, step, components_queue, mandatory_values_buffer, optional_values_buffer, pipeline_output\n    ):\n        \"\"\"\n        Stores a snapshot of this step into the self.debug dictionary of the pipeline.\n        \"\"\"\n        mermaid_graph = _convert_for_debug(deepcopy(self.graph))\n        self._debug[step] = {\n            \"time\": datetime.datetime.now(),\n            \"components_queue\": components_queue,\n            \"mandatory_values_buffer\": mandatory_values_buffer,\n            \"optional_values_buffer\": optional_values_buffer,\n            \"pipeline_output\": pipeline_output,\n            \"diagram\": mermaid_graph,\n        }\n\n    def _clear_visits_count(self):\n        \"\"\"\n        Make sure all nodes's visits count is zero.\n        \"\"\"\n        for node in self.graph.nodes:\n            self.graph.nodes[node][\"visits\"] = 0\n\n    def _check_max_loops(self, component_name: str):\n        \"\"\"\n        Verify whether this component run too many times.\n        \"\"\"\n        if self.graph.nodes[component_name][\"visits\"] &gt; self.max_loops_allowed:\n            raise PipelineMaxLoops(\n                f\"Maximum loops count ({self.max_loops_allowed}) exceeded for component '{component_name}'.\"\n            )\n\n    def _add_value_to_buffers(\n        self,\n        value: Any,\n        connection: Connection,\n        components_queue: List[str],\n        mandatory_values_buffer: Dict[Connection, Any],\n        optional_values_buffer: Dict[Connection, Any],\n    ):\n        \"\"\"\n        Given a value and the connection it is being sent on, it updates the buffers and the components queue.\n        \"\"\"\n        if connection.is_mandatory:\n            mandatory_values_buffer[connection] = value\n            if connection.receiver and connection.receiver not in components_queue:\n                components_queue.append(connection.receiver)\n        else:\n            optional_values_buffer[connection] = value\n\n    def _ready_to_run(\n        self, component_name: str, mandatory_values_buffer: Dict[Connection, Any], components_queue: List[str]\n    ) -&gt; bool:\n        \"\"\"\n        Returns True if a component is ready to run, False otherwise.\n        \"\"\"\n        connections_with_value = set(conn for conn in mandatory_values_buffer.keys() if conn.receiver == component_name)\n        expected_connections = set(self._mandatory_connections[component_name])\n        if expected_connections.issubset(connections_with_value):\n            logger.debug(\"Component '%s' is ready to run. All mandatory values were received.\", component_name)\n            return True\n\n        # Collect the missing values still being computed we need to wait for\n        missing_connections: Set[Connection] = expected_connections - connections_with_value\n        connections_to_wait = []\n        for missing_conn in missing_connections:\n            if any(\n                networkx.has_path(self.graph, component_to_run, missing_conn.sender)\n                for component_to_run in components_queue\n            ):\n                connections_to_wait.append(missing_conn)\n\n        if not connections_to_wait:\n            # None of the missing values are needed to visit this part of the graph: we can run the component\n            logger.debug(\n                \"Component '%s' is ready to run. A variadic input parameter received all the expected values.\",\n                component_name,\n            )\n            return True\n\n        # Component can't run, waiting for the values needed by `connections_to_wait`\n        logger.debug(\n            \"Component '%s' is not ready to run, some values are still missing: %s\",\n            component_name,\n            connections_to_wait,\n        )\n        # Put the component back in the queue\n        components_queue.append(component_name)\n        return False\n\n    def _extract_inputs_from_buffer(self, component_name: str, buffer: Dict[Connection, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extract a component's input values from one of the value buffers.\n        \"\"\"\n        inputs = defaultdict(list)\n        connections: List[Connection] = []\n\n        for connection in buffer.keys():\n            if connection.receiver == component_name:\n                connections.append(connection)\n\n        for key in connections:\n            value = buffer.pop(key)\n            if key.receiver_socket:\n                if key.receiver_socket.is_variadic:\n                    inputs[key.receiver_socket.name].append(value)\n                else:\n                    inputs[key.receiver_socket.name] = value\n        return inputs\n\n    def _run_component(self, name: str, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Once we're confident this component is ready to run, run it and collect the output.\n        \"\"\"\n        self.graph.nodes[name][\"visits\"] += 1\n        instance = self.graph.nodes[name][\"instance\"]\n        try:\n            logger.info(\"* Running %s\", name)\n            logger.debug(\"   '%s' inputs: %s\", name, inputs)\n\n            outputs = instance.run(**inputs)\n\n            # Unwrap the output\n            logger.debug(\"   '%s' outputs: %s\\n\", name, outputs)\n\n            # Make sure the component returned a dictionary\n            if not isinstance(outputs, dict):\n                raise PipelineRuntimeError(\n                    f\"Component '{name}' returned a value of type '{_type_name(type(outputs))}' instead of a dict. \"\n                    \"Components must always return dictionaries: check the the documentation.\"\n                )\n\n        except Exception as e:\n            raise PipelineRuntimeError(\n                f\"{name} raised '{e.__class__.__name__}: {e}' \\nInputs: {inputs}\\n\\n\"\n                \"See the stacktrace above for more information.\"\n            ) from e\n\n        return outputs\n\n    def _collect_targets(self, component_name: str, socket_name: str) -&gt; List[Connection]:\n        \"\"\"\n        Given a component and an output socket name, return a list of Connections\n        for which they represent the sender. Used to route output.\n        \"\"\"\n        return [\n            connection\n            for connection in self._connections\n            if connection.sender == component_name\n            and connection.sender_socket\n            and connection.sender_socket.name == socket_name\n        ]\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Equal pipelines share every metadata, node and edge, but they're not required to use the same node instances: this allows pipeline saved and then loaded back to be equal to themselves.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def __eq__(self, other) -&gt; bool:\n    \"\"\"\n    Equal pipelines share every metadata, node and edge, but they're not required to use\n    the same node instances: this allows pipeline saved and then loaded back to be equal to themselves.\n    \"\"\"\n    if (\n        not isinstance(other, type(self))\n        or not getattr(self, \"metadata\") == getattr(other, \"metadata\")\n        or not getattr(self, \"max_loops_allowed\") == getattr(other, \"max_loops_allowed\")\n        or not hasattr(self, \"graph\")\n        or not hasattr(other, \"graph\")\n    ):\n        return False\n\n    return (\n        self.graph.adj == other.graph.adj\n        and self._comparable_nodes_list(self.graph) == self._comparable_nodes_list(other.graph)\n        and self.graph.graph == other.graph.graph\n    )\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.__init__","title":"<code>__init__(metadata=None, max_loops_allowed=100, debug_path=Path('.canals_debug/'))</code>","text":"<p>Creates the Pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>arbitrary dictionary to store metadata about this pipeline. Make sure all the values contained in this dictionary can be serialized and deserialized if you wish to save this pipeline to file with <code>save_pipelines()/load_pipelines()</code>.</p> <code>None</code> <code>max_loops_allowed</code> <code>int</code> <p>how many times the pipeline can run the same node before throwing an exception.</p> <code>100</code> <code>debug_path</code> <code>Union[Path, str]</code> <p>when debug is enabled in <code>run()</code>, where to save the debug data.</p> <code>Path('.canals_debug/')</code> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def __init__(\n    self,\n    metadata: Optional[Dict[str, Any]] = None,\n    max_loops_allowed: int = 100,\n    debug_path: Union[Path, str] = Path(\".canals_debug/\"),\n):\n    \"\"\"\n    Creates the Pipeline.\n\n    Args:\n        metadata: arbitrary dictionary to store metadata about this pipeline. Make sure all the values contained in\n            this dictionary can be serialized and deserialized if you wish to save this pipeline to file with\n            `save_pipelines()/load_pipelines()`.\n        max_loops_allowed: how many times the pipeline can run the same node before throwing an exception.\n        debug_path: when debug is enabled in `run()`, where to save the debug data.\n    \"\"\"\n    self.metadata = metadata or {}\n    self.max_loops_allowed = max_loops_allowed\n    self.graph = networkx.MultiDiGraph()\n    self._connections: List[Connection] = []\n    self._mandatory_connections: Dict[str, List[Connection]] = defaultdict(list)\n    self._debug: Dict[int, Dict[str, Any]] = {}\n    self._debug_path = Path(debug_path)\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.add_component","title":"<code>add_component(name, instance)</code>","text":"<p>Create a component for the given component. Components are not connected to anything by default: use <code>Pipeline.connect()</code> to connect components together.</p> <p>Component names must be unique, but component instances can be reused if needed.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the component.</p> required <code>instance</code> <code>Component</code> <p>the component instance.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if a component with the same name already exists</p> <code>PipelineValidationError</code> <p>if the given instance is not a Canals component</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def add_component(self, name: str, instance: Component) -&gt; None:\n    \"\"\"\n    Create a component for the given component. Components are not connected to anything by default:\n    use `Pipeline.connect()` to connect components together.\n\n    Component names must be unique, but component instances can be reused if needed.\n\n    Args:\n        name: the name of the component.\n        instance: the component instance.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: if a component with the same name already exists\n        PipelineValidationError: if the given instance is not a Canals component\n    \"\"\"\n    # Component names are unique\n    if name in self.graph.nodes:\n        raise ValueError(f\"A component named '{name}' already exists in this pipeline: choose another name.\")\n\n    # Components can't be named `_debug`\n    if name == \"_debug\":\n        raise ValueError(\"'_debug' is a reserved name for debug output. Choose another name.\")\n\n    # Component instances must be components\n    if not isinstance(instance, Component):\n        raise PipelineValidationError(\n            f\"'{type(instance)}' doesn't seem to be a component. Is this class decorated with @component?\"\n        )\n\n    # Create the component's input and output sockets\n    input_sockets = getattr(instance, \"__canals_input__\", {})\n    output_sockets = getattr(instance, \"__canals_output__\", {})\n\n    # Add component to the graph, disconnected\n    logger.debug(\"Adding component '%s' (%s)\", name, instance)\n    self.graph.add_node(\n        name, instance=instance, input_sockets=input_sockets, output_sockets=output_sockets, visits=0\n    )\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.connect","title":"<code>connect(connect_from, connect_to)</code>","text":"<p>Connects two components together. All components to connect must exist in the pipeline. If connecting to an component that has several output connections, specify the inputs and output names as 'component_name.connections_name'.</p> <p>Parameters:</p> Name Type Description Default <code>connect_from</code> <code>str</code> <p>the component that delivers the value. This can be either just a component name or can be in the format <code>component_name.connection_name</code> if the component has multiple outputs.</p> required <code>connect_to</code> <code>str</code> <p>the component that receives the value. This can be either just a component name or can be in the format <code>component_name.connection_name</code> if the component has multiple inputs.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>PipelineConnectError</code> <p>if the two components cannot be connected (for example if one of the components is not present in the pipeline, or the connections don't match by type, and so on).</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def connect(self, connect_from: str, connect_to: str) -&gt; None:\n    \"\"\"\n    Connects two components together. All components to connect must exist in the pipeline.\n    If connecting to an component that has several output connections, specify the inputs and output names as\n    'component_name.connections_name'.\n\n    Args:\n        connect_from: the component that delivers the value. This can be either just a component name or can be\n            in the format `component_name.connection_name` if the component has multiple outputs.\n        connect_to: the component that receives the value. This can be either just a component name or can be\n            in the format `component_name.connection_name` if the component has multiple inputs.\n\n    Returns:\n        None\n\n    Raises:\n        PipelineConnectError: if the two components cannot be connected (for example if one of the components is\n            not present in the pipeline, or the connections don't match by type, and so on).\n    \"\"\"\n    # Edges may be named explicitly by passing 'node_name.edge_name' to connect().\n    sender, sender_socket_name = parse_connect_string(connect_from)\n    receiver, receiver_socket_name = parse_connect_string(connect_to)\n\n    # Get the nodes data.\n    try:\n        from_sockets = self.graph.nodes[sender][\"output_sockets\"]\n    except KeyError as exc:\n        raise ValueError(f\"Component named {sender} not found in the pipeline.\") from exc\n    try:\n        to_sockets = self.graph.nodes[receiver][\"input_sockets\"]\n    except KeyError as exc:\n        raise ValueError(f\"Component named {receiver} not found in the pipeline.\") from exc\n\n    # If the name of either socket is given, get the socket\n    sender_socket: Optional[OutputSocket] = None\n    if sender_socket_name:\n        sender_socket = from_sockets.get(sender_socket_name)\n        if not sender_socket:\n            raise PipelineConnectError(\n                f\"'{connect_from} does not exist. \"\n                f\"Output connections of {sender} are: \"\n                + \", \".join([f\"{name} (type {_type_name(socket.type)})\" for name, socket in from_sockets.items()])\n            )\n\n    receiver_socket: Optional[InputSocket] = None\n    if receiver_socket_name:\n        receiver_socket = to_sockets.get(receiver_socket_name)\n        if not receiver_socket:\n            raise PipelineConnectError(\n                f\"'{connect_to} does not exist. \"\n                f\"Input connections of {receiver} are: \"\n                + \", \".join([f\"{name} (type {_type_name(socket.type)})\" for name, socket in to_sockets.items()])\n            )\n\n    # Look for a matching connection among the possible ones.\n    # Note that if there is more than one possible connection but two sockets match by name, they're paired.\n    sender_socket_candidates: List[OutputSocket] = [sender_socket] if sender_socket else list(from_sockets.values())\n    receiver_socket_candidates: List[InputSocket] = (\n        [receiver_socket] if receiver_socket else list(to_sockets.values())\n    )\n\n    connection = Connection.from_list_of_sockets(\n        sender, sender_socket_candidates, receiver, receiver_socket_candidates\n    )\n\n    # Connection must be valid on both sender/receiver sides\n    if (\n        not connection.sender_socket\n        or not connection.receiver_socket\n        or not connection.sender\n        or not connection.receiver\n    ):\n        raise PipelineConnectError(\"Connection must have both sender and receiver: {connection}\")\n\n    # Create the connection\n    logger.debug(\n        \"Connecting '%s.%s' to '%s.%s'\",\n        connection.sender,\n        connection.sender_socket.name,\n        connection.receiver,\n        connection.receiver_socket.name,\n    )\n\n    self.graph.add_edge(\n        connection.sender,\n        connection.receiver,\n        key=f\"{connection.sender_socket.name}/{connection.receiver_socket.name}\",\n        conn_type=_type_name(connection.sender_socket.type),\n        from_socket=connection.sender_socket,\n        to_socket=connection.receiver_socket,\n    )\n\n    self._connections.append(connection)\n    if connection.is_mandatory:\n        self._mandatory_connections[connection.receiver].append(connection)\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.draw","title":"<code>draw(path, engine='mermaid-image')</code>","text":"<p>Draws the pipeline. Requires either <code>graphviz</code> as a system dependency, or an internet connection for Mermaid. Run <code>pip install canals[graphviz]</code> or <code>pip install canals[mermaid]</code> to install missing dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>where to save the diagram.</p> required <code>engine</code> <code>RenderingEngines</code> <p>which format to save the graph as. Accepts 'graphviz', 'mermaid-text', 'mermaid-image'. Default is 'mermaid-image'.</p> <code>'mermaid-image'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>if <code>engine='graphviz'</code> and <code>pygraphviz</code> is not installed.</p> <code>HTTPConnectionError</code> <p>(and similar) if the internet connection is down or other connection issues.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def draw(self, path: Path, engine: RenderingEngines = \"mermaid-image\") -&gt; None:\n    \"\"\"\n    Draws the pipeline. Requires either `graphviz` as a system dependency, or an internet connection for Mermaid.\n    Run `pip install canals[graphviz]` or `pip install canals[mermaid]` to install missing dependencies.\n\n    Args:\n        path: where to save the diagram.\n        engine: which format to save the graph as. Accepts 'graphviz', 'mermaid-text', 'mermaid-image'.\n            Default is 'mermaid-image'.\n\n    Returns:\n        None\n\n    Raises:\n        ImportError: if `engine='graphviz'` and `pygraphviz` is not installed.\n        HTTPConnectionError: (and similar) if the internet connection is down or other connection issues.\n    \"\"\"\n    _draw(graph=networkx.MultiDiGraph(self.graph), path=path, engine=engine)\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.from_dict","title":"<code>from_dict(data, **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates a Pipeline instance from a dictionary. A sample <code>data</code> dictionary could be formatted like so: <pre><code>{\n    \"metadata\": {\"test\": \"test\"},\n    \"max_loops_allowed\": 100,\n    \"components\": {\n        \"add_two\": {\n            \"type\": \"AddFixedValue\",\n            \"init_parameters\": {\"add\": 2},\n        },\n        \"add_default\": {\n            \"type\": \"AddFixedValue\",\n            \"init_parameters\": {\"add\": 1},\n        },\n        \"double\": {\n            \"type\": \"Double\",\n        },\n    },\n    \"connections\": [\n        {\"sender\": \"add_two.result\", \"receiver\": \"double.value\"},\n        {\"sender\": \"double.value\", \"receiver\": \"add_default.value\"},\n    ],\n}\n</code></pre></p> <p>Supported kwargs: <code>components</code>: a dictionary of {name: instance} to reuse instances of components instead of creating new ones.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef from_dict(cls: Type[T], data: Dict[str, Any], **kwargs) -&gt; T:\n    \"\"\"\n    Creates a Pipeline instance from a dictionary.\n    A sample `data` dictionary could be formatted like so:\n    ```\n    {\n        \"metadata\": {\"test\": \"test\"},\n        \"max_loops_allowed\": 100,\n        \"components\": {\n            \"add_two\": {\n                \"type\": \"AddFixedValue\",\n                \"init_parameters\": {\"add\": 2},\n            },\n            \"add_default\": {\n                \"type\": \"AddFixedValue\",\n                \"init_parameters\": {\"add\": 1},\n            },\n            \"double\": {\n                \"type\": \"Double\",\n            },\n        },\n        \"connections\": [\n            {\"sender\": \"add_two.result\", \"receiver\": \"double.value\"},\n            {\"sender\": \"double.value\", \"receiver\": \"add_default.value\"},\n        ],\n    }\n    ```\n\n    Supported kwargs:\n    `components`: a dictionary of {name: instance} to reuse instances of components instead of creating new ones.\n    \"\"\"\n    metadata = data.get(\"metadata\", {})\n    max_loops_allowed = data.get(\"max_loops_allowed\", 100)\n    debug_path = Path(data.get(\"debug_path\", \".canals_debug/\"))\n    pipe = cls(metadata=metadata, max_loops_allowed=max_loops_allowed, debug_path=debug_path)\n    components_to_reuse = kwargs.get(\"components\", {})\n    for name, component_data in data.get(\"components\", {}).items():\n        if name in components_to_reuse:\n            # Reuse an instance\n            instance = components_to_reuse[name]\n        else:\n            if \"type\" not in component_data:\n                raise PipelineError(f\"Missing 'type' in component '{name}'\")\n\n            if component_data[\"type\"] not in component.registry:\n                try:\n                    # Import the module first...\n                    module, _ = component_data[\"type\"].rsplit(\".\", 1)\n                    logger.debug(\"Trying to import %s\", module)\n                    importlib.import_module(module)\n                    # ...then try again\n                    if component_data[\"type\"] not in component.registry:\n                        raise PipelineError(\n                            f\"Successfully imported module {module} but can't find it in the component registry.\"\n                            \"This is unexpected and most likely a bug.\"\n                        )\n                except (ImportError, PipelineError) as e:\n                    raise PipelineError(f\"Component '{component_data['type']}' not imported.\") from e\n\n            # Create a new one\n            component_class = component.registry[component_data[\"type\"]]\n            instance = component_from_dict(component_class, component_data)\n        pipe.add_component(name=name, instance=instance)\n\n    for connection in data.get(\"connections\", []):\n        if \"sender\" not in connection:\n            raise PipelineError(f\"Missing sender in connection: {connection}\")\n        if \"receiver\" not in connection:\n            raise PipelineError(f\"Missing receiver in connection: {connection}\")\n        pipe.connect(connect_from=connection[\"sender\"], connect_to=connection[\"receiver\"])\n\n    return pipe\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.get_component","title":"<code>get_component(name)</code>","text":"<p>Returns an instance of a component.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the component</p> required <p>Returns:</p> Type Description <code>Component</code> <p>The instance of that component.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if a component with that name is not present in the pipeline.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def get_component(self, name: str) -&gt; Component:\n    \"\"\"\n    Returns an instance of a component.\n\n    Args:\n        name: the name of the component\n\n    Returns:\n        The instance of that component.\n\n    Raises:\n        ValueError: if a component with that name is not present in the pipeline.\n    \"\"\"\n    try:\n        return self.graph.nodes[name][\"instance\"]\n    except KeyError as exc:\n        raise ValueError(f\"Component named {name} not found in the pipeline.\") from exc\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.inputs","title":"<code>inputs()</code>","text":"<p>Returns a dictionary containing the inputs of a pipeline. Each key in the dictionary corresponds to a component name, and its value is another dictionary that describes the input sockets of that component, including their types and whether they are optional.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary where each key is a pipeline component name and each value is a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>inputs sockets of that component.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def inputs(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Returns a dictionary containing the inputs of a pipeline. Each key in the dictionary\n    corresponds to a component name, and its value is another dictionary that describes the\n    input sockets of that component, including their types and whether they are optional.\n\n    Returns:\n        A dictionary where each key is a pipeline component name and each value is a dictionary of\n        inputs sockets of that component.\n    \"\"\"\n    inputs = {\n        comp: {socket.name: {\"type\": socket.type, \"is_mandatory\": socket.is_mandatory} for socket in data}\n        for comp, data in find_pipeline_inputs(self.graph).items()\n        if data\n    }\n    return inputs\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.outputs","title":"<code>outputs()</code>","text":"<p>Returns a dictionary containing the outputs of a pipeline. Each key in the dictionary corresponds to a component name, and its value is another dictionary that describes the output sockets of that component.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary where each key is a pipeline component name and each value is a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>output sockets of that component.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def outputs(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Returns a dictionary containing the outputs of a pipeline. Each key in the dictionary\n    corresponds to a component name, and its value is another dictionary that describes the\n    output sockets of that component.\n\n    Returns:\n        A dictionary where each key is a pipeline component name and each value is a dictionary of\n        output sockets of that component.\n    \"\"\"\n    outputs = {\n        comp: {socket.name: {\"type\": socket.type} for socket in data}\n        for comp, data in find_pipeline_outputs(self.graph).items()\n        if data\n    }\n    return outputs\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.run","title":"<code>run(data, debug=False)</code>","text":"<p>Runs the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>the inputs to give to the input components of the Pipeline.</p> required <code>debug</code> <code>bool</code> <p>whether to collect and return debug information.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary with the outputs of the output components of the Pipeline.</p> <p>Raises:</p> Type Description <code>PipelineRuntimeError</code> <p>if the any of the components fail or return unexpected output.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def run(self, data: Dict[str, Any], debug: bool = False) -&gt; Dict[str, Any]:  # pylint: disable=too-many-locals\n    \"\"\"\n    Runs the pipeline.\n\n    Args:\n        data: the inputs to give to the input components of the Pipeline.\n        debug: whether to collect and return debug information.\n\n    Returns:\n        A dictionary with the outputs of the output components of the Pipeline.\n\n    Raises:\n        PipelineRuntimeError: if the any of the components fail or return unexpected output.\n    \"\"\"\n    self._clear_visits_count()\n    data = validate_pipeline_input(self.graph, input_values=data)\n    logger.info(\"Pipeline execution started.\")\n\n    self._debug = {}\n    if debug:\n        logger.info(\"Debug mode ON.\")\n        os.makedirs(\"debug\", exist_ok=True)\n\n    logger.debug(\n        \"Mandatory connections:\\n%s\",\n        \"\\n\".join(\n            f\" - {component}: {', '.join([str(s) for s in sockets])}\"\n            for component, sockets in self._mandatory_connections.items()\n        ),\n    )\n\n    self.warm_up()\n\n    # Prepare the inputs buffers and components queue\n    components_queue: List[str] = []\n    mandatory_values_buffer: Dict[Connection, Any] = {}\n    optional_values_buffer: Dict[Connection, Any] = {}\n    pipeline_output: Dict[str, Dict[str, Any]] = defaultdict(dict)\n\n    for node_name, input_data in data.items():\n        for socket_name, value in input_data.items():\n            # Make a copy of the input value so components don't need to\n            # take care of mutability.\n            value = deepcopy(value)\n            connection = Connection(\n                None, None, node_name, self.graph.nodes[node_name][\"input_sockets\"][socket_name]\n            )\n            self._add_value_to_buffers(\n                value, connection, components_queue, mandatory_values_buffer, optional_values_buffer\n            )\n\n    # *** PIPELINE EXECUTION LOOP ***\n    step = 0\n    while components_queue:  # pylint: disable=too-many-nested-blocks\n        step += 1\n        if debug:\n            self._record_pipeline_step(\n                step, components_queue, mandatory_values_buffer, optional_values_buffer, pipeline_output\n            )\n\n        component_name = components_queue.pop(0)\n        logger.debug(\"&gt; Queue at step %s: %s %s\", step, component_name, components_queue)\n        self._check_max_loops(component_name)\n\n        # **** RUN THE NODE ****\n        if not self._ready_to_run(component_name, mandatory_values_buffer, components_queue):\n            continue\n\n        inputs = {\n            **self._extract_inputs_from_buffer(component_name, mandatory_values_buffer),\n            **self._extract_inputs_from_buffer(component_name, optional_values_buffer),\n        }\n        outputs = self._run_component(name=component_name, inputs=dict(inputs))\n\n        # **** PROCESS THE OUTPUT ****\n        for socket_name, value in outputs.items():\n            targets = self._collect_targets(component_name, socket_name)\n            if not targets:\n                pipeline_output[component_name][socket_name] = value\n            else:\n                for target in targets:\n                    self._add_value_to_buffers(\n                        value, target, components_queue, mandatory_values_buffer, optional_values_buffer\n                    )\n\n    if debug:\n        self._record_pipeline_step(\n            step + 1, components_queue, mandatory_values_buffer, optional_values_buffer, pipeline_output\n        )\n        os.makedirs(self._debug_path, exist_ok=True)\n        with open(self._debug_path / \"data.json\", \"w\", encoding=\"utf-8\") as datafile:\n            json.dump(self._debug, datafile, indent=4, default=str)\n        pipeline_output[\"_debug\"] = self._debug  # type: ignore\n\n    logger.info(\"Pipeline executed successfully.\")\n    return dict(pipeline_output)\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.to_dict","title":"<code>to_dict()</code>","text":"<p>Returns this Pipeline instance as a dictionary. This is meant to be an intermediate representation but it can be also used to save a pipeline to file.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns this Pipeline instance as a dictionary.\n    This is meant to be an intermediate representation but it can be also used to save a pipeline to file.\n    \"\"\"\n    components = {}\n    for name, instance in self.graph.nodes(data=\"instance\"):  # type:ignore\n        components[name] = component_to_dict(instance)\n\n    connections = []\n    for sender, receiver, edge_data in self.graph.edges.data():\n        sender_socket = edge_data[\"from_socket\"].name\n        receiver_socket = edge_data[\"to_socket\"].name\n        connections.append({\"sender\": f\"{sender}.{sender_socket}\", \"receiver\": f\"{receiver}.{receiver_socket}\"})\n    return {\n        \"metadata\": self.metadata,\n        \"max_loops_allowed\": self.max_loops_allowed,\n        \"components\": components,\n        \"connections\": connections,\n    }\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.pipeline.Pipeline.warm_up","title":"<code>warm_up()</code>","text":"<p>Make sure all nodes are warm.</p> <p>It's the node's responsibility to make sure this method can be called at every <code>Pipeline.run()</code> without re-initializing everything.</p> Source code in <code>canals/pipeline/pipeline.py</code> <pre><code>def warm_up(self):\n    \"\"\"\n    Make sure all nodes are warm.\n\n    It's the node's responsibility to make sure this method can be called at every `Pipeline.run()`\n    without re-initializing everything.\n    \"\"\"\n    for node in self.graph.nodes:\n        if hasattr(self.graph.nodes[node][\"instance\"], \"warm_up\"):\n            logger.info(\"Warming up component %s...\", node)\n            self.graph.nodes[node][\"instance\"].warm_up()\n</code></pre>"},{"location":"api-docs/pipeline/#canals.pipeline.validation.validate_pipeline_input","title":"<code>validate_pipeline_input(graph, input_values)</code>","text":"<p>Make sure the pipeline is properly built and that the input received makes sense. Returns the input values, validated and updated at need.</p> Source code in <code>canals/pipeline/validation.py</code> <pre><code>def validate_pipeline_input(graph: networkx.MultiDiGraph, input_values: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Make sure the pipeline is properly built and that the input received makes sense.\n    Returns the input values, validated and updated at need.\n    \"\"\"\n    if not any(sockets for sockets in find_pipeline_inputs(graph).values()):\n        raise PipelineValidationError(\"This pipeline has no inputs.\")\n\n    # Make sure the input keys are all nodes of the pipeline\n    unknown_components = [key for key in input_values.keys() if not key in graph.nodes]\n    if unknown_components:\n        all_inputs = describe_pipeline_inputs_as_string(graph)\n        raise ValueError(\n            f\"Pipeline received data for unknown component(s): {', '.join(unknown_components)}\\n\\n{all_inputs}\"\n        )\n\n    # Make sure all necessary sockets are connected\n    _validate_input_sockets_are_connected(graph, input_values)\n\n    # Make sure that the pipeline input is only sent to nodes that won't receive data from other nodes\n    _validate_nodes_receive_only_expected_input(graph, input_values)\n\n    return input_values\n</code></pre>"},{"location":"api-docs/testing/","title":"Testing","text":""},{"location":"api-docs/testing/#canals.testing.factory.component_class","title":"<code>component_class(name, input_types=None, output_types=None, output=None, bases=None, extra_fields=None)</code>","text":"<p>Utility class to create a Component class with the given name and input and output types.</p> <p>If <code>output</code> is set but <code>output_types</code> is not, <code>output_types</code> will be set to the types of the values in <code>output</code>. Though if <code>output_types</code> is set but <code>output</code> is not the component's <code>run</code> method will return a dictionary of the same keys as <code>output_types</code> all with a value of None.</p>"},{"location":"api-docs/testing/#canals.testing.factory.component_class--usage","title":"Usage","text":"<p>Create a component class with default input and output types: <pre><code>MyFakeComponent = component_class_factory(\"MyFakeComponent\")\ncomponent = MyFakeComponent()\noutput = component.run(value=1)\nassert output == {\"value\": None}\n</code></pre></p> <p>Create a component class with an \"value\" input of type <code>int</code> and with a \"value\" output of <code>10</code>: <pre><code>MyFakeComponent = component_class_factory(\n    \"MyFakeComponent\",\n    input_types={\"value\": int},\n    output={\"value\": 10}\n)\ncomponent = MyFakeComponent()\noutput = component.run(value=1)\nassert output == {\"value\": 10}\n</code></pre></p> <p>Create a component class with a custom base class: <pre><code>MyFakeComponent = component_class_factory(\n    \"MyFakeComponent\",\n    bases=(MyBaseClass,)\n)\ncomponent = MyFakeComponent()\nassert isinstance(component, MyBaseClass)\n</code></pre></p> <p>Create a component class with an extra field <code>my_field</code>: <pre><code>MyFakeComponent = component_class_factory(\n    \"MyFakeComponent\",\n    extra_fields={\"my_field\": 10}\n)\ncomponent = MyFakeComponent()\nassert component.my_field == 10\n</code></pre></p> <p>Args: name: Name of the component class input_types: Dictionary of string and type that defines the inputs of the component,     if set to None created component will expect a single input \"value\" of Any type.     Defaults to None. output_types: Dictionary of string and type that defines the outputs of the component,     if set to None created component will return a single output \"value\" of NoneType and None value.     Defaults to None. output: Actual output dictionary returned by the created component run,     is set to None it will return a dictionary of string and None values.     Keys will be the same as the keys of output_types. Defaults to None. bases: Base classes for this component, if set to None only base is object. Defaults to None. extra_fields: Extra fields for the Component, defaults to None.</p> <p>:return: A class definition that can be used as a component.</p> Source code in <code>canals/testing/factory.py</code> <pre><code>def component_class(\n    name: str,\n    input_types: Optional[Dict[str, Any]] = None,\n    output_types: Optional[Dict[str, Any]] = None,\n    output: Optional[Dict[str, Any]] = None,\n    bases: Optional[Tuple[type, ...]] = None,\n    extra_fields: Optional[Dict[str, Any]] = None,\n) -&gt; Type[Component]:\n    \"\"\"\n    Utility class to create a Component class with the given name and input and output types.\n\n    If `output` is set but `output_types` is not, `output_types` will be set to the types of the values in `output`.\n    Though if `output_types` is set but `output` is not the component's `run` method will return a dictionary\n    of the same keys as `output_types` all with a value of None.\n\n    ### Usage\n\n    Create a component class with default input and output types:\n    ```python\n    MyFakeComponent = component_class_factory(\"MyFakeComponent\")\n    component = MyFakeComponent()\n    output = component.run(value=1)\n    assert output == {\"value\": None}\n    ```\n\n    Create a component class with an \"value\" input of type `int` and with a \"value\" output of `10`:\n    ```python\n    MyFakeComponent = component_class_factory(\n        \"MyFakeComponent\",\n        input_types={\"value\": int},\n        output={\"value\": 10}\n    )\n    component = MyFakeComponent()\n    output = component.run(value=1)\n    assert output == {\"value\": 10}\n    ```\n\n    Create a component class with a custom base class:\n    ```python\n    MyFakeComponent = component_class_factory(\n        \"MyFakeComponent\",\n        bases=(MyBaseClass,)\n    )\n    component = MyFakeComponent()\n    assert isinstance(component, MyBaseClass)\n    ```\n\n    Create a component class with an extra field `my_field`:\n    ```python\n    MyFakeComponent = component_class_factory(\n        \"MyFakeComponent\",\n        extra_fields={\"my_field\": 10}\n    )\n    component = MyFakeComponent()\n    assert component.my_field == 10\n    ```\n\n    Args:\n    name: Name of the component class\n    input_types: Dictionary of string and type that defines the inputs of the component,\n        if set to None created component will expect a single input \"value\" of Any type.\n        Defaults to None.\n    output_types: Dictionary of string and type that defines the outputs of the component,\n        if set to None created component will return a single output \"value\" of NoneType and None value.\n        Defaults to None.\n    output: Actual output dictionary returned by the created component run,\n        is set to None it will return a dictionary of string and None values.\n        Keys will be the same as the keys of output_types. Defaults to None.\n    bases: Base classes for this component, if set to None only base is object. Defaults to None.\n    extra_fields: Extra fields for the Component, defaults to None.\n\n    :return: A class definition that can be used as a component.\n    \"\"\"\n    if input_types is None:\n        input_types = {\"value\": Any}\n    if output_types is None and output is not None:\n        output_types = {key: type(value) for key, value in output.items()}\n    elif output_types is None:\n        output_types = {\"value\": type(None)}\n\n    def init(self):\n        component.set_input_types(self, **input_types)\n        component.set_output_types(self, **output_types)\n\n    # Both arguments are necessary to correctly define\n    # run but pylint doesn't like that we don't use them.\n    # It's fine ignoring the warning here.\n    def run(self, **kwargs):  # pylint: disable=unused-argument\n        if output is not None:\n            return output\n        return {name: None for name in output_types.keys()}\n\n    def to_dict(self):\n        return default_to_dict(self)\n\n    def from_dict(cls, data: Dict[str, Any]):\n        return default_from_dict(cls, data)\n\n    fields = {\n        \"__init__\": init,\n        \"run\": run,\n        \"to_dict\": to_dict,\n        \"from_dict\": classmethod(from_dict),\n    }\n    if extra_fields is not None:\n        fields = {**fields, **extra_fields}\n\n    if bases is None:\n        bases = (object,)\n\n    cls = type(name, bases, fields)\n    return component(cls)\n</code></pre>"},{"location":"concepts/components/","title":"Components","text":"<p>In order to be recognized as components and work in a Pipeline, Components must follow the contract below.</p>"},{"location":"concepts/components/#requirements","title":"Requirements","text":""},{"location":"concepts/components/#component-decorator","title":"<code>@component</code> decorator","text":"<p>All component classes must be decorated with the <code>@component</code> decorator. This allows Canals to discover them.</p>"},{"location":"concepts/components/#componentinput","title":"<code>@component.input</code>","text":"<p>All components must decorate one single method with the <code>@component.input</code> decorator. This method must return a dataclass, which will be used as structure of the input of the component.</p> <p>For example, if the node is expecting a list of Documents, the fields of the returned dataclass should be <code>documents: List[Document]</code>. Note that you don't need to decorate the dataclass youself: <code>@component.input</code> will add the decorator for you.</p> <p>Here is an example of such method:</p> <pre><code>@component.input\ndef input(self):\n    class Input:\n        value: int\n        add: int\n\n    return Input\n</code></pre> <p>Defaults are allowed, as much as default factories and other dataclass properties.</p> <p>By default <code>@component.input</code> sets <code>None</code> as default for all fields, regardless of their definition: this gives you the possibility of passing a part of the input to the pipeline without defining every field of the component. For example, using the above definition, you can create an Input dataclass as:</p> <pre><code>self.input(add=3)\n</code></pre> <p>and the resulting dataclass will look like <code>Input(value=None, add=3)</code>.</p> <p>However, if you don't explicitly define them as Optionals, Pipeline will make sure to collect all the values of this dataclass before calling the <code>run()</code> method, making them in practice non-optional.</p> <p>If you instead define a specific field as Optional in the dataclass, then Pipeline will not wait for them, and will run the component as soon as all the non-optional fields have received a value or, if all fields are optional, if at least one of them received it.</p> <p>This behavior allows Canals to define loops by not waiting on both incoming inputs of the entry component of the loop, and instead running as soon as at least one of them receives a value.</p>"},{"location":"concepts/components/#componentoutput","title":"<code>@component.output</code>","text":"<p>All components must decorate one single method with the <code>@component.output</code> decorator. This method must return a dataclass, which will be used as structure of the output of the component.</p> <p>For example, if the node is producing a list of Documents, the fields of the returned dataclass should be <code>documents: List[Document]</code>. Note that you don't need to decorate the dataclass youself: <code>@component.output</code> will add the decorator for you.</p> <p>Here is an example of such method:</p> <pre><code>@component.output\ndef output(self):\n    class Output:\n        value: int\n\n    return Output\n</code></pre> <p>Defaults are allowed, as much as default factories and other dataclass properties.</p>"},{"location":"concepts/components/#__init__self-kwargs","title":"<code>__init__(self, **kwargs)</code>","text":"<p>Optional method.</p> <p>Components may have an <code>__init__</code> method where they define:</p> <ul> <li> <p><code>self.defaults = {parameter_name: parameter_default_value, ...}</code>:     All values defined here will be sent to the <code>run()</code> method when the Pipeline calls it.     If any of these parameters is also receiving input from other components, those have precedence.     This collection of values is supposed to replace the need for default values in <code>run()</code> and make them     dynamically configurable. Keep in mind that only these defaults will count at runtime: defaults given to     the <code>Input</code> dataclass (see above) will be ignored.</p> </li> <li> <p><code>self.init_parameters = {same parameters that the __init__ method received}</code>:     In this dictionary you can store any state the components wish to be persisted when they are saved.     These values will be given to the <code>__init__</code> method of a new instance when the pipeline is loaded.     Note that by default the <code>@component</code> decorator saves the arguments automatically.     However, if a component sets their own <code>init_parameters</code> manually in <code>__init__()</code>, that will be used instead.     Note: all of the values contained here must be JSON serializable. Serialize them manually if needed.</p> </li> </ul> <p>Components should take only \"basic\" Python types as parameters of their <code>__init__</code> function, or iterables and dictionaries containing only such values. Anything else (objects, functions, etc) will raise an exception at init time. If there's the need for such values, consider serializing them to a string.</p> <p>(TODO explain how to use classes and functions in init. In the meantime see <code>test/components/test_accumulate.py</code>)</p> <p>The <code>__init__</code> must be extrememly lightweight, because it's a frequent operation during the construction and validation of the pipeline. If a component has some heavy state to initialize (models, backends, etc...) refer to the <code>warm_up()</code> method.</p>"},{"location":"concepts/components/#warm_upself","title":"<code>warm_up(self)</code>","text":"<p>Optional method.</p> <p>This method is called by Pipeline before the graph execution. Make sure to avoid double-initializations, because Pipeline will not keep track of which components it called <code>warm_up()</code> on.</p>"},{"location":"concepts/components/#runself-data","title":"<code>run(self, data)</code>","text":"<p>Mandatory method.</p> <p>This is the method where the main functionality of the component should be carried out. It's called by <code>Pipeline.run()</code>.</p> <p>When the component should run, Pipeline will call this method with an instance of the dataclass returned by the method decorated with <code>@component.input</code>. This dataclass contains:</p> <ul> <li>all the input values coming from other components connected to it,</li> <li>if any is missing, the corresponding value defined in <code>self.defaults</code>, if it exists.</li> </ul> <p><code>run()</code> must return a single instance of the dataclass declared through the method decorated with <code>@component.output</code>.</p>"},{"location":"concepts/components/#example-components","title":"Example components","text":"<p>Here is an example of a simple component that adds a fixed value to its input and returns their sum.</p> <pre><code>from typing import Optional\nfrom canals.component import component\n\n@component\nclass AddFixedValue:\n    \"\"\"\n    Adds the value of `add` to `value`. If not given, `add` defaults to 1.\n    \"\"\"\n\n    @component.input  # type: ignore\n    def input(self):\n        class Input:\n            value: int\n            add: int\n\n        return Input\n\n    @component.output  # type: ignore\n    def output(self):\n        class Output:\n            value: int\n\n        return Output\n\n    def __init__(self, add: Optional[int] = 1):\n        if add:\n            self.defaults = {\"add\": add}\n\n    def run(self, data):\n        return self.output(value=data.value + data.add)\n</code></pre> <p>See <code>tests/sample_components</code> for examples of more complex components with variable inputs and output, and so on.</p>"},{"location":"concepts/concepts/","title":"Core concepts","text":"<p>Canals is a component orchestration engine. It can be used to connect a group of smaller objects, called Components, that perform well-defined tasks into a network, called Pipeline, to achieve a larger goal.</p> <p>Components are Python objects that can execute a task, like reading a file, performing calculations, or making API calls. Canals connects these objects together: it builds a graph of components and takes care of managing their execution order, making sure that each object receives the input it expects from the other components of the pipeline at the right time.</p> <p>Canals relies on two main concepts: Components and Pipelines.</p>"},{"location":"concepts/concepts/#what-is-a-component","title":"What is a Component?","text":"<p>A Component is a Python class that performs a well-defined task: for example a REST API call, a mathematical operation, a data trasformation, writing something to a file or a database, and so on.</p> <p>To be recognized as a Component by Canals, a Python class needs to respect these rules:</p> <ol> <li>Must be decorated with the <code>@component</code> decorator.</li> <li>Have a <code>run()</code> method that accepts a <code>data</code> parameter of type <code>ComponentInput</code> return a single object of type <code>ComponentOutput</code>.</li> </ol> <p>For example, the following is a Component that sums up two numbers:</p> <pre><code>from dataclasses import dataclass\nfrom canals.component import component, ComponentInput, ComponentOutput\n\n@component\nclass AddFixedValue:\n    \"\"\"\n    Adds the value of `add` to `value`. If not given, `add` defaults to 1.\n    \"\"\"\n\n    @component.input\n    def input(self):\n        class Input:\n            value: int\n            add: int\n\n        return Input\n\n    @component.output\n    def output(self):\n        class Output:\n            value: int\n\n        return Output\n\n    def __init__(self, add: Optional[int] = 1):\n        if add:\n            self.defaults = {\"add\": add}\n\n    def run(self, data):\n        return self.output(value=data.value + data.add)\n</code></pre> <p>We will see the details of all of these requirements below.</p>"},{"location":"concepts/concepts/#what-is-a-pipeline","title":"What is a Pipeline?","text":"<p>A Pipeline is a network of Components. Pipelines define what components receive and send output to which other, makes sure all the connections are valid, and takes care of calling the component's <code>run()</code> method in the right order.</p> <p>Pipeline connects compoonents together through so-called connections, which are the edges of the pipeline graph. Pipeline is going to make sure that all the connections are valid based on the inputs and output that Components have declared.</p> <p>For example, if a component produces a value of type <code>List[Document]</code> and another component expects an input of type <code>List[Document]</code>, Pipeline will be able to connect them. Otherwise, it will raise an exception.</p> <p>This is a simple example of how a Pipeline is created:</p> <pre><code>from canals.pipeline import Pipeline\n\n# Some Canals components\nfrom my_components import AddFixedValue, MultiplyBy\n\npipeline = Pipeline()\n\n# Components can be initialized as standalone objects.\n# These instances can be added to the Pipeline in several places.\nmultiplication = MultiplyBy(multiply_by=2)\naddition = AddFixedValue(add=1)\n\n# Components are added with a name and an component\npipeline.add_component(\"double\", multiplication)\npipeline.add_component(\"add_one\", addition)\npipeline.add_component(\"add_one_again\", addition)  # Component instances can be reused\npipeline.add_component(\"add_two\", AddFixedValue(add=2))\n\n# Connect the components together\npipeline.connect(connect_from=\"double\", connect_to=\"add_one\")\npipeline.connect(connect_from=\"add_one\", connect_to=\"add_one_again\")\npipeline.connect(connect_from=\"add_one_again\", connect_to=\"add_two\")\n\n# Pipeline can be drawn\npipeline.draw(\"pipeline.jpg\")\n\n# Pipelines are run by giving them the data that the input nodes expect.\nresults = pipeline.run(data={\"double\": multiplication.input(value=1)})\n\nprint(results)\n\n# prints {\"add_two\": AddFixedValue.Output(value=6)}\n</code></pre> <p>This is how the pipeline's graph looks like:</p> <pre><code>graph TD;\ndouble -- value -&gt; value --&gt; add_one\nadd_one -- value -&gt; value --&gt; add_one_again\nadd_one_again -- value -&gt; value --&gt; add_two\nIN([input]) -- value --&gt; double\nadd_two -- value --&gt; OUT([output])</code></pre>"},{"location":"concepts/pipelines/","title":"Pipelines","text":"<p>Canals aims to support pipelines of (close to) arbitrary complexity. It currently supports a variety of different topologies, such as:</p> <ul> <li>Simple linear pipelines</li> <li>Branching pipelines where all or only some branches are executed</li> <li>Pipelines merging a variable number of inputs, depending on decisions taken upstream</li> <li>Simple loops</li> <li>Multiple entry components, either alternative or parallel</li> <li>Multiple exit components, either alternative or parallel</li> </ul> <p>Check the pipeline's test suite for some examples.</p>"},{"location":"concepts/pipelines/#validation","title":"Validation","text":"<p>Pipeline performs validation on the connection type level: when calling <code>Pipeline.connect()</code>, it uses the <code>@component.input</code> and <code>@component.output</code> dataclass fields to make sure that the connection is possible.</p> <p>On top of this, specific connections can be specified with the syntax <code>component_name.input_or_output_field</code>.</p> <p>For example, let's imagine we have two components with the following I/O declared:</p> <pre><code>@component\nclass ComponentA:\n\n    @component.input\n    def input(self):\n        class Input:\n            input_value: int\n\n        return Input\n\n    @component.output\n    def output(self):\n        class Output:\n            output_value: str\n\n        return Output\n\n    def run(self, data):\n        return self.output(intermediate_value=\"hello\")\n\n@component\nclass ComponentB:\n\n    @component.input\n    def input(self):\n        class Input:\n            input_value: str\n\n        return Input\n\n    @component.output\n    def output(self):\n        class Output:\n            output_value: List[str]\n\n        return Output\n\n    def run(self, data):\n        return self.output(output_value=[\"h\", \"e\", \"l\", \"l\", \"o\"])\n</code></pre> <p>This is the behavior of <code>Pipeline.connect()</code>:</p> <pre><code>pipeline.add_component('component_a', ComponentA())\npipeline.add_component('component_b', ComponentB())\n\n# All of these succeeds\npipeline.connect('component_a', 'component_b')\npipeline.connect('component_a.output_value', 'component_b')\npipeline.connect('component_a', 'component_b.input_value')\npipeline.connect('component_a.output_value', 'component_b.input_value')\n</code></pre> <p>These, instead, fail:</p> <pre><code>pipeline.connect('component_a', 'component_a')\n# canals.errors.PipelineConnectError: Cannot connect 'component_a' with 'component_a': no matching connections available.\n# 'component_a':\n#  - output_value (str)\n# 'component_a':\n#  - input_value (int, available)\n\npipeline.connect('component_b', 'component_a')\n# canals.errors.PipelineConnectError: Cannot connect 'component_b' with 'component_a': no matching connections available.\n# 'component_b':\n#  - output_value (List[str])\n# 'component_a':\n#  - input_value (int, available)\n</code></pre> <p>In addition, components names are validated:</p> <pre><code>pipeline.connect('component_a', 'component_c')\n# ValueError: Component named component_c not found in the pipeline.\n</code></pre> <p>Just like input and output names, when stated:</p> <pre><code>pipeline.connect('component_a.input_value', 'component_b')\n# canals.errors.PipelineConnectError: 'component_a.typo does not exist. Output connections of component_a are: output_value (type str)\n\npipeline.connect('component_a', 'component_b.output_value')\n# canals.errors.PipelineConnectError: 'component_b.output_value does not exist. Input connections of component_b are: input_value (type str)\n</code></pre>"},{"location":"concepts/pipelines/#save-and-load","title":"Save and Load","text":"<p>Pipelines can be serialized to Python dictionaries, that can be then dumped to JSON or to any other suitable format, like YAML, TOML, HCL, etc. These pipelines can then be loaded back.</p> <p>Here is an example of Pipeline saving and loading:</p> <pre><code>from haystack.pipelines import Pipeline, save_pipelines, load_pipelines\n\npipe1 = Pipeline()\npipe2 = Pipeline()\n\n# .. assemble the pipelines ...\n\n# Save the pipelines\nsave_pipelines(\n    pipelines={\n        \"pipe1\": pipe1,\n        \"pipe2\": pipe2,\n    },\n    path=\"my_pipelines.json\",\n    _writer=json.dumps\n)\n\n# Load the pipelines\nnew_pipelines = load_pipelines(\n    path=\"my_pipelines.json\",\n    _reader=json.loads\n)\n\nassert new_pipelines[\"pipe1\"] == pipe1\nassert new_pipelines[\"pipe2\"] == pipe2\n</code></pre> <p>Note how the save/load functions accept a <code>_writer</code>/<code>_reader</code> function: this choice frees us from committing strongly to a specific template language, and although a default will be set (be it YAML, TOML, HCL or anything else) the decision can be overridden by passing another explicit reader/writer function to the <code>save_pipelines</code>/<code>load_pipelines</code> functions.</p> <p>This is how the resulting file will look like, assuming a JSON writer was chosen.</p> <p><code>my_pipeline.json</code></p> <pre><code>{\n    \"pipelines\": {\n        \"pipe1\": {\n            # All the components that would be added with a\n            # Pipeline.add_component() call\n            \"components\": {\n                \"first_addition\": {\n                    \"type\": \"AddValue\",\n                    \"init_parameters\": {\n                        \"add\": 1\n                    },\n                },\n                \"double\": {\n                    \"type\": \"Double\",\n                    \"init_parameters\": {}\n                },\n                \"second_addition\": {\n                    \"type\": \"AddValue\",\n                    \"init_parameters\": {\n                        \"add\": 1\n                    },\n                },\n                # This is how instances of the same component are reused\n                \"third_addition\": {\n                    \"refer_to\": \"pipe1.first_addition\"\n                },\n            },\n            # All the components that would be made with a\n            # Pipeline.connect() call\n            \"connections\": [\n                (\"first_addition\", \"double\", \"value/value\"),\n                (\"double\", \"second_addition\", \"value/value\"),\n                (\"second_addition\", \"third_addition\", \"value/value\"),\n            ],\n            # All other Pipeline.__init__() parameters go here.\n            \"metadata\": {\"type\": \"test pipeline\", \"author\": \"me\"},\n            \"max_loops_allowed\": 100,\n        },\n        \"pipe2\": {\n            \"components\": {\n                \"first_addition\": {\n                    # We can reference components from other pipelines too!\n                    \"refer_to\": \"pipe1.first_addition\",\n                },\n                \"double\": {\n                    \"type\": \"Double\",\n                    \"init_parameters\": {}\n                },\n                \"second_addition\": {\n                    \"refer_to\": \"pipe1.second_addition\"\n                },\n            },\n            \"connections\": [\n                (\"first_addition\", \"double\", \"value/value\"),\n                (\"double\", \"second_addition\", \"value/value\"),\n            ],\n            \"metadata\": {\"type\": \"another test pipeline\", \"author\": \"you\"},\n            \"max_loops_allowed\": 100,\n        },\n    },\n    # A list of \"dependencies\" for the application.\n    # Used to ensure all external components are present when loading.\n    \"dependencies\": [\"my_custom_components_module\"],\n}\n</code></pre>"}]}